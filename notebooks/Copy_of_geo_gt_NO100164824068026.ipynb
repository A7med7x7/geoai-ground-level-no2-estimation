{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8770223,
          "sourceType": "datasetVersion",
          "datasetId": 5270243
        }
      ],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'zindi-geoai-ground-level-no2-estimation-challenge:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5270243%2F8770223%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240714%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240714T134437Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D31f3bd823e21cedac111978a1e8d9524d468e12ea624e97e1568e04c793706e994e0ff9940d6e19e24a0e365e270f135f1eca47e67e4d5d13695d1cb703bfbc688fe2397bd8ffd1024ec64f7e5edc59b8dd6f25ad68240375c2ccafc1ae35a8da0374dcc71c0eef26a115040520d215071fa7c5bb8442099b5691ddee9f77e07985f70e7c6af2ff4ff5238f38582b5549ed42fdae8662597d519b8f33a405bd753ec946d20eec12316341410bf35aaa71352ed9d18fd94405663f610647685d0ff1b8ff26a320b940cb9bc1ed0994c40ac1fe5f9f9ff00df0131686d88cf3cc644e65d3eabef7ff8aaef7f9e658cf8dd37cc12e73a4cdba6ea01f73108ad2a61'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "863HHJcDA2kz",
        "outputId": "3174f623-2290-4fa9-9370-08095417df0f"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading zindi-geoai-ground-level-no2-estimation-challenge, 3366877 bytes compressed\n",
            "[==================================================] 3366877 bytes downloaded\n",
            "Downloaded and uncompressed: zindi-geoai-ground-level-no2-estimation-challenge\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importings"
      ],
      "metadata": {
        "id": "GbxChli6A2k1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- mutual information\n",
        "- search for the catboost regressor on kaggle"
      ],
      "metadata": {
        "id": "86s-4DVxA2k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd                                    # for data\n",
        "import numpy as np                                     # for math\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error         # Regressortion metric\n",
        "from sklearn.model_selection import GroupKFold,KFold, TimeSeriesSplit   # for validation\n",
        "from sklearn.preprocessing import LabelEncoder         # for encoding\n",
        "import sklearn.manifold._t_sne as tsne                 # for t_sne\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import TimeSeriesSplit# for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from matplotlib import rcParams\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from scipy.stats import rankdata\n",
        "import xgboost as xgb\n",
        "from sklearn.cluster import KMeans\n",
        "from path import Path\n",
        "path = Path('/kaggle/input/zindi-geoai-ground-level-no2-estimation-challenge')\n",
        "train = pd.read_csv(path /'Train.csv')\n",
        "test = pd.read_csv(path /'Test.csv')\n",
        "groups = train['ID']\n",
        "test_id = test['ID_Zindi']\n",
        "pd.options.display.max_columns = 200\n",
        "#train = train.dropna(axis=0)\n",
        "#test = test.dropna(axis=0)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:07.741877Z",
          "iopub.execute_input": "2024-07-14T13:27:07.74247Z",
          "iopub.status.idle": "2024-07-14T13:27:07.982266Z",
          "shell.execute_reply.started": "2024-07-14T13:27:07.742426Z",
          "shell.execute_reply": "2024-07-14T13:27:07.980998Z"
        },
        "trusted": true,
        "id": "wfN_LZD2A2k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.dropna(subset=['GT_NO2'])\n",
        "train.isnull().sum()\n",
        "#in order to the catboost model to be evaluated"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:07.984154Z",
          "iopub.execute_input": "2024-07-14T13:27:07.984596Z",
          "iopub.status.idle": "2024-07-14T13:27:08.02151Z",
          "shell.execute_reply.started": "2024-07-14T13:27:07.984557Z",
          "shell.execute_reply": "2024-07-14T13:27:08.020297Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg8w61uzA2k3",
        "outputId": "356fea83-3ab7-43ea-fca9-c4675db61478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID_Zindi                  0\n",
              "Date                      0\n",
              "ID                        0\n",
              "LAT                       0\n",
              "LON                       0\n",
              "Precipitation             0\n",
              "LST                   37594\n",
              "AAI                   12118\n",
              "CloudFraction         12118\n",
              "NO2_strat             12118\n",
              "NO2_total             12118\n",
              "NO2_trop              33429\n",
              "TropopausePressure    12118\n",
              "GT_NO2                    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "PTg6C2AgA2k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"agg_funcs = ['mean', 'std', 'min', 'max']\n",
        "\n",
        "def add_aggregated_features(dataset, columns, funcs):\n",
        "    dataset = dataset.copy()\n",
        "    for column in columns:\n",
        "        agg_features_id = dataset.groupby('ID')[column].agg(funcs)\n",
        "        agg_features_id.columns = [f'{column}_{agg_func}_agg_ID' for agg_func in funcs]\n",
        "        dataset = dataset.merge(agg_features_id, on='ID')\n",
        "    return dataset\n",
        "train = add_aggregated_features(train, columns_to_aggregate, agg_funcs)\n",
        "test = add_aggregated_features(test, columns_to_aggregate, agg_funcs)\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:08.022665Z",
          "iopub.execute_input": "2024-07-14T13:27:08.022969Z",
          "iopub.status.idle": "2024-07-14T13:27:08.02988Z",
          "shell.execute_reply.started": "2024-07-14T13:27:08.022945Z",
          "shell.execute_reply": "2024-07-14T13:27:08.028643Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "kKBAekNnA2k4",
        "outputId": "f8c79dde-10d8-4e75-beaf-0b93c16d539b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"agg_funcs = ['mean', 'std', 'min', 'max'] \\n\\ndef add_aggregated_features(dataset, columns, funcs):\\n    dataset = dataset.copy()\\n    for column in columns:\\n        agg_features_id = dataset.groupby('ID')[column].agg(funcs)\\n        agg_features_id.columns = [f'{column}_{agg_func}_agg_ID' for agg_func in funcs]\\n        dataset = dataset.merge(agg_features_id, on='ID')\\n    return dataset\\ntrain = add_aggregated_features(train, columns_to_aggregate, agg_funcs)\\ntest = add_aggregated_features(test, columns_to_aggregate, agg_funcs)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_feats = train.select_dtypes(include=['float'])\n",
        "kmeans = KMeans(n_clusters=2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:08.032298Z",
          "iopub.execute_input": "2024-07-14T13:27:08.03265Z",
          "iopub.status.idle": "2024-07-14T13:27:08.040844Z",
          "shell.execute_reply.started": "2024-07-14T13:27:08.03262Z",
          "shell.execute_reply": "2024-07-14T13:27:08.039708Z"
        },
        "trusted": true,
        "id": "dxYxybSVA2k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lat_min, lat_max = 44.92469405, 45.88973369\n",
        "lon_min, lon_max = 8.736496578, 12.59068235\n",
        "\n",
        "num_clusters_lat = 3\n",
        "num_clusters_lon = 4\n",
        "lat_step = (lat_max - lat_min) / num_clusters_lat\n",
        "lon_step = (lon_max - lon_min) / num_clusters_lon\n",
        "def assign_clusters(row, lat_step, lon_step, lat_min, lon_min):\n",
        "    lat_cluster = int((row['LAT'] - lat_min) / lat_step)\n",
        "    lon_cluster = int((row['LON'] - lon_min) / lon_step)\n",
        "    return lat_cluster, lon_cluster\n",
        "for dataset in (train, test):\n",
        "    dataset[['lat_cluster', 'lon_cluster']] = dataset.apply(\n",
        "        assign_clusters, axis=1, result_type='expand',\n",
        "        lat_step=lat_step, lon_step=lon_step, lat_min=lat_min, lon_min=lon_min\n",
        "    )\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:08.041968Z",
          "iopub.execute_input": "2024-07-14T13:27:08.042411Z",
          "iopub.status.idle": "2024-07-14T13:27:11.331717Z",
          "shell.execute_reply.started": "2024-07-14T13:27:08.042375Z",
          "shell.execute_reply": "2024-07-14T13:27:11.330532Z"
        },
        "trusted": true,
        "id": "Q4vR1sbXA2k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"train = train.sort_values('Date').reset_index(drop=True)\n",
        "test = test.sort_values('Date').reset_index(drop=True)\n",
        "\"\"\"\n",
        "for df in (train,test):\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    #df['Day'] =  df['Date'].dt.day\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    #df['target_month'] = df['Month'].map(df[['Month','GT_NO2']].groupby('Month')['GT_NO2'].mean())\n",
        "    #df['Month_start'] = df['Date'].dt.is_month_start\n",
        "    #df['Year'] =  df['Date'].dt.year\n",
        "    #df.set_index(df['Date'],inplace=True)\n",
        "    #df['DayOfWeek'] =  df['Date'].dt.dayofweek\n",
        "    df.drop(columns=['ID_Zindi'],inplace=True)\n",
        "\n",
        "#Linterations\n",
        "for df in(train,test):\n",
        "    df['NO2_strat_lag_1'] = df['NO2_strat'].shift(1)\n",
        "    df['NO2_strat_lag_7'] = df['NO2_strat'].shift(7)\n",
        "    df['Precipitation_inter1'] = df['TropopausePressure'] + df['Precipitation']\n",
        "    df['Precipitation_log'] =  np.log(df['Precipitation'] + 1e-9)\n",
        "    df['CloudFraction_diff'] = df['CloudFraction'] / df['NO2_strat']\n",
        "    df['CloudFraction_diff2'] = df['CloudFraction'] / df['NO2_total']\n",
        "    #['Precipitation_fractional'] = df['Precipitation'] * 0.00001\n",
        "    df['TropopausePressure_fractional'] = round(df['TropopausePressure'] * 0.00001,2)\n",
        "    #df['cluster'] = kmeans.fit_transform(df[['LAT', 'LON']])\n",
        "    df['cloud_fraction1'] = df['CloudFraction'] % df['NO2_strat']\n",
        "    df['cloud_fraction2'] = df['CloudFraction'] % df['NO2_total']\n",
        "\n",
        "#for df in(train,test):\n",
        "    #Rolling (Moving Average)\n",
        "    #df['feature_rolling_3_mean'] = df['TropopausePressure'].rolling(5).mean()\n",
        "    #df['feature_rolling_7_mean'] = df['TropopausePressure'].rolling(7).mean()\n",
        "    #df['feature_rolling_7_std'] = df['TropopausePressure'].rolling(7).std()\n",
        "    #df['NO2_strat_rolling7'] = df['NO2_strat'].rolling(7).mean()\n",
        "    #df['NO2_strat_rolling30'] = df['NO2_strat'].rolling(30).mean()\n",
        "    #df['NO2_strat_rolling2'] = df['NO2_strat'].rolling(9).mean()\n",
        "#statsitics of similar variables\n",
        "\n",
        "\"\"\"def MeanSd(feature1, feature2):\n",
        "    for dataset in (train,test):\n",
        "        dataset[\"SD\" + feature1] = dataset[[feature1,feature2]].std(axis=1)\n",
        "        dataset[\"MEAN\" + feature1] = dataset[[feature1,feature2]].mean(axis=1)\n",
        "\n",
        "MeanSd('NO2_trop','NO2_total')\n",
        "MeanSd('LON','LAT')\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:11.360855Z",
          "iopub.execute_input": "2024-07-14T13:27:11.361234Z",
          "iopub.status.idle": "2024-07-14T13:27:11.529134Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.361203Z",
          "shell.execute_reply": "2024-07-14T13:27:11.527859Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "w-UjVVwUA2k5",
        "outputId": "edcc6275-9cb7-4f0f-a3b7-4c6cf0cf674f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-0647ad694997>:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n",
            "<ipython-input-53-0647ad694997>:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def MeanSd(feature1, feature2):\\n    for dataset in (train,test):\\n        dataset[\"SD\" + feature1] = dataset[[feature1,feature2]].std(axis=1)\\n        dataset[\"MEAN\" + feature1] = dataset[[feature1,feature2]].mean(axis=1)\\n\\nMeanSd(\\'NO2_trop\\',\\'NO2_total\\')\\nMeanSd(\\'LON\\',\\'LAT\\')'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for df in(train,test):\n",
        "    #df['LST_mean_60'] = df['LST'].rolling(60).mean()\n",
        "    #df['Prec_mean_60'] = df['Precipitation'].rolling(60).mean()\n",
        "    #df['AII_mean'] = df['AAI'].rolling(60).mean()\n",
        "    #df['no2_total'] = df['NO2_total'].rolling(60).min()\n",
        "\n",
        "    #df['feature_rolling_3_mean'] = df['TropopausePressure'].rolling(5).mean()\n",
        "    #df['feature_rolling_7_mean'] = df['TropopausePressure'].rolling(7).mean()\n",
        "    #df['feature_rolling_7_std'] = df['TropopausePressure'].rolling(7).std()\n",
        "    #df['NO2_strat_rolling7'] = df['NO2_strat'].rolling(7).mean()\n",
        "    #df['NO2_strat_rolling30'] = df['NO2_strat'].rolling(30).mean()\n",
        "    #df['NO2_strat_rolling2'] = df['NO2_strat'].rolling(9).mean()\n",
        "\n",
        "\n",
        "def rolling(feature):\n",
        "    for dataset in (train,test):\n",
        "        #dataset[f\"{feature}_rolling_mean_60\"] = dataset[feature].rolling(60).mean()\n",
        "        dataset[f\"{feature}_rolling_max_60\"] = dataset[feature].rolling(60).max()\n",
        "        #dataset[f\"{feature}_rolling_min_60\"] = dataset[feature].rolling(60).min()\n",
        "\n",
        "\n",
        "rolling('NO2_trop')\n",
        "rolling('NO2_total')\n",
        "rolling('TropopausePressure')\n",
        "rolling('CloudFraction')\n",
        "#rolling('AAI')\n",
        "#rolling('LST')\n",
        "rolling('Precipitation')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:11.530748Z",
          "iopub.execute_input": "2024-07-14T13:27:11.531198Z",
          "iopub.status.idle": "2024-07-14T13:27:11.565633Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.53115Z",
          "shell.execute_reply": "2024-07-14T13:27:11.564625Z"
        },
        "trusted": true,
        "id": "oXVjF7-PA2k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"columns_to_aggregate = [\n",
        "    'LAT', 'LON', 'distance', 'Precipitation_rolling_max_60', 'NO2_strat',\n",
        "    'NO2_total', 'NO2_total_rolling_max_60', 'AAI', 'LST', 'CloudFraction',\n",
        "    'TropopausePressure', 'NO2_trop', 'TropopausePressure_rolling_max_60',\n",
        "    'CloudFraction_rolling_max_60', 'Precipitation', 'NO2_trop_rolling_max_60'\n",
        "]\n",
        "for df in (train,test):\n",
        "    for col in columns_to_aggregate:\n",
        "        df[f'{col}_monthly_mean'] = df.groupby('Month')[col].transform('mean')\n",
        "        df[f'{col}_monthly_sum'] = df.groupby('Month')[col].transform('sum')\n",
        "        df[f'{col}_monthly_max'] = df.groupby('Month')[col].transform('max')\n",
        "        df[f'{col}_monthly_min'] = df.groupby('Month')[col].transform('min')\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.set_index('Date')\n",
        "for df in (train,test):\n",
        "    for col in columns_to_aggregate:\n",
        "        df[f'{col}_rolling_mean_30'] = df[col].rolling(window=30).mean()\n",
        "        df[f'{col}_rolling_sum_30'] = df[col].rolling(window=30).sum()\n",
        "        df[f'{col}_rolling_max_30'] = df[col].rolling(window=30).max()\n",
        "        df[f'{col}_rolling_min_30'] = df[col].rolling(window=30).min()\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:11.566991Z",
          "iopub.execute_input": "2024-07-14T13:27:11.567512Z",
          "iopub.status.idle": "2024-07-14T13:27:11.575445Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.567469Z",
          "shell.execute_reply": "2024-07-14T13:27:11.574302Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "m2869ucNA2k6",
        "outputId": "02efce7b-65c1-440b-d620-ca545fe33bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"columns_to_aggregate = [\\n    'LAT', 'LON', 'distance', 'Precipitation_rolling_max_60', 'NO2_strat', \\n    'NO2_total', 'NO2_total_rolling_max_60', 'AAI', 'LST', 'CloudFraction', \\n    'TropopausePressure', 'NO2_trop', 'TropopausePressure_rolling_max_60', \\n    'CloudFraction_rolling_max_60', 'Precipitation', 'NO2_trop_rolling_max_60'\\n]\\nfor df in (train,test):\\n    for col in columns_to_aggregate:\\n        df[f'{col}_monthly_mean'] = df.groupby('Month')[col].transform('mean')\\n        df[f'{col}_monthly_sum'] = df.groupby('Month')[col].transform('sum')\\n        df[f'{col}_monthly_max'] = df.groupby('Month')[col].transform('max')\\n        df[f'{col}_monthly_min'] = df.groupby('Month')[col].transform('min')\\n\\ndf['Date'] = pd.to_datetime(df['Date'])\\ndf = df.set_index('Date')\\nfor df in (train,test):\\n    for col in columns_to_aggregate:\\n        df[f'{col}_rolling_mean_30'] = df[col].rolling(window=30).mean()\\n        df[f'{col}_rolling_sum_30'] = df[col].rolling(window=30).sum()\\n        df[f'{col}_rolling_max_30'] = df[col].rolling(window=30).max()\\n        df[f'{col}_rolling_min_30'] = df[col].rolling(window=30).min()\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Values & Encoding"
      ],
      "metadata": {
        "id": "I8bwdOoRA2k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groups = train['ID']\n",
        "for df in(train,test):\n",
        "    df.drop(columns=[\"Date\",'ID','Precipitation','CloudFraction','AAI','lat_cluster'], axis=1,inplace=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "for df in(train,test):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = le.fit_transform(df[col])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:11.579374Z",
          "iopub.execute_input": "2024-07-14T13:27:11.579757Z",
          "iopub.status.idle": "2024-07-14T13:27:11.597165Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.579716Z",
          "shell.execute_reply": "2024-07-14T13:27:11.596076Z"
        },
        "trusted": true,
        "id": "9Q7DTxHNA2k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CV and Modeling"
      ],
      "metadata": {
        "id": "KELbHAFzA2k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:11.644483Z",
          "iopub.execute_input": "2024-07-14T13:27:11.644891Z",
          "iopub.status.idle": "2024-07-14T13:27:11.660164Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.644856Z",
          "shell.execute_reply": "2024-07-14T13:27:11.65905Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpuC0D5VA2k8",
        "outputId": "d1e58508-31d4-4719-db50-c88b0596d1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['LAT', 'LON', 'LST', 'NO2_strat', 'NO2_total', 'NO2_trop',\n",
              "       'TropopausePressure', 'GT_NO2', 'lon_cluster', 'Month',\n",
              "       'NO2_strat_lag_1', 'NO2_strat_lag_7', 'Precipitation_inter1',\n",
              "       'Precipitation_log', 'CloudFraction_diff', 'CloudFraction_diff2',\n",
              "       'TropopausePressure_fractional', 'cloud_fraction1', 'cloud_fraction2',\n",
              "       'NO2_trop_rolling_max_60', 'NO2_total_rolling_max_60',\n",
              "       'TropopausePressure_rolling_max_60', 'CloudFraction_rolling_max_60',\n",
              "       'Precipitation_rolling_max_60'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop"
      ],
      "metadata": {
        "id": "NkLkYE7PNB6a",
        "outputId": "e9ed855b-b830-4b7b-d024-2308af6a1d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stop' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-3dd9cae63c82>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model =  LGBMRegressor(random_state=7)\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRFRegressor\n",
        "#model = XGBRFRegressor(random_state=7)\n",
        "model = CatBoostRegressor(random_state=7)\n",
        "\n",
        "#model = XGBRegressor(random_state= 7)\n",
        "n_splits = 5\n",
        "n = train['GT_NO2'].count()\n",
        "num_bins = int(1 + np.log2(n))\n",
        "cv = GroupKFold(n_splits=n_splits)\n",
        "\n",
        "def validate(trainset, testset, target_col):\n",
        "\n",
        "    model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    print('std:', valid_testset.std())\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    print('score:', score)\n",
        "\n",
        "    return score\n",
        "stds = []\n",
        "rmse = []\n",
        "\n",
        "for train_idx, test_idx in cv.split(train.drop(columns='GT_NO2'), train['GT_NO2'], groups=groups):\n",
        "    train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "    stds.append(test_v['GT_NO2'].std())\n",
        "    rmse.append(validate(train_v, test_v, 'GT_NO2'))\n",
        "\n",
        "print('RMSE:', np.array(rmse).mean())\n",
        "print('RMSE std:', np.array(rmse).std())\n",
        "print('Standard Deviations:', stds)\n",
        "print('RMSEs Deviations:', rmse)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:11.661856Z",
          "iopub.execute_input": "2024-07-14T13:27:11.662298Z",
          "iopub.status.idle": "2024-07-14T13:27:58.266503Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.662257Z",
          "shell.execute_reply": "2024-07-14T13:27:58.264922Z"
        },
        "trusted": true,
        "id": "HHaRM8N7A2k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ",./fuiogn"
      ],
      "metadata": {
        "id": "plzHI6cnKA3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.155311364148783 catboost leaderboard best s far 9.525616244\n",
        "#9.920018431939276 catboost and added the day features - probably it overfits - indeed\n",
        "#9.43514150859055 removing all missing values leaderboard 11.9315086\n",
        "#10.139203906659699\n",
        "#10.201862135497844 the effect of mutual information\n",
        "#9.476601295728365\n",
        "#10.025125392453269"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.268096Z",
          "iopub.execute_input": "2024-07-14T13:27:58.268549Z",
          "iopub.status.idle": "2024-07-14T13:27:58.273759Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.268508Z",
          "shell.execute_reply": "2024-07-14T13:27:58.272513Z"
        },
        "trusted": true,
        "id": "v_2Fi9rJA2k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11.581549939504997 with month ,\n",
        "#12.246931687286574\n",
        "#12.235259341356834\n",
        "#12.231869616518921\n",
        "#12.126333476852636\n",
        "#11.728753513719479\n",
        "#11.724263538742687\n",
        "#11.728753513719479\n",
        "#11.711277895239192\n",
        "#11.694341231865433\n",
        "#11.671982426722977\n",
        "#11.642227859275334 without the mean - with the custamized means of rolling\n",
        "#11.314985626750522\n",
        "#11.314985626750522\n",
        "#11.312413659993586\n",
        "#11.135932399851725\n",
        "#11.728753513719479 best so far features = ['LAT', 'Month','NO2_trop_rolling_max_60','NO2_total_rolling_max_60','TropopausePressure_rolling_max_60','CloudFraction_rolling_max_60','Precipitation_rolling_max_60']\n",
        "#11.40320255907346 with the ID LB = 9.571700669\n",
        "#11.597286160951526 the distance feature"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.275324Z",
          "iopub.execute_input": "2024-07-14T13:27:58.275771Z",
          "iopub.status.idle": "2024-07-14T13:27:58.284828Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.275732Z",
          "shell.execute_reply": "2024-07-14T13:27:58.283586Z"
        },
        "trusted": true,
        "id": "FK5Eig4FA2k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib, os,sys\n",
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Removed feature: {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "\n",
        "def lofo_analysis(train, target_col, groups, n_splits):\n",
        "    base_rmse = []\n",
        "    for train_idx, test_idx in cv.split(train.drop(columns=target_col), train[target_col], groups=groups):\n",
        "        train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "        base_rmse.append(validate(train_v, test_v, target_col))\n",
        "    base_score = np.array(base_rmse).mean()\n",
        "    print('Base RMSE:', base_score)\n",
        "\n",
        "    feature_importances = {}\n",
        "    for col in train.drop(columns=target_col).columns:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train.drop(columns=[target_col, col]), train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v.drop(columns=col), test_v.drop(columns=col), target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} removed, RMSE: {feature_rmse}')\n",
        "\n",
        "    bad_features = [col for col in feature_importances if feature_importances[col] > base_score]\n",
        "    good_features = [col for col in feature_importances if feature_importances[col] <= base_score]\n",
        "\n",
        "    print('Good features:', good_features)\n",
        "    print('Bad features:', bad_features)\n",
        "\n",
        "    return good_features, bad_features, feature_importances\n",
        "\n",
        "good_features, bad_features, feature_importances = lofo_analysis(train, 'GT_NO2', groups, n_splits)\n",
        "\n",
        "print('Feature importances:')\n",
        "for feature, importance in feature_importances.items():\n",
        "    print(f'{feature}: {importance}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.286156Z",
          "iopub.execute_input": "2024-07-14T13:27:58.286545Z",
          "iopub.status.idle": "2024-07-14T13:27:58.304718Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.286509Z",
          "shell.execute_reply": "2024-07-14T13:27:58.303412Z"
        },
        "trusted": true,
        "id": "eaUv4zbZA2k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"def rolling(feature):\n",
        "    for dataset in (train,test):\n",
        "        dataset['Rolling_3'] = dataset[feature].rolling(3).mean()\n",
        "        dataset['Rolling_5'] = dataset[feature].rolling(5).mean()\n",
        "\n",
        "        dataset[f\"{feature}_rolling_mean_60\"] = dataset.rolling(60).mean()[feature]\n",
        "        dataset[f\"{feature}_rolling_max_60\"] = dataset.rolling(60).max()[feature]\n",
        "        dataset[f\"{feature}_rolling_min_60\"] = dataset.rolling(60).min()[feature]\n",
        "\n",
        "        dataset[f\"{feature}_rolling_mean_30\"] = dataset.rolling(30).mean()[feature]\n",
        "        dataset[f\"{feature}_rolling_max_30\"] = dataset.rolling(30).max()[feature]\n",
        "        dataset[f\"{feature}_rolling_min_30\"] = dataset.rolling(30).min()[feature]\n",
        "\n",
        "        dataset[f\"{feature}_rolling_mean_10\"] = dataset.rolling(10).mean()[feature]\n",
        "        dataset[f\"{feature}_rolling_max_10\"] = dataset.rolling(10).max()[feature]\n",
        "        dataset[f\"{feature}_rolling_min_10\"] = dataset.rolling(10).min()[feature]\n",
        "\n",
        "rolling('NO2_strat')\n",
        "rolling('NO2_total')\"\"\""
      ],
      "metadata": {
        "id": "g4kwEXnUV29z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.306385Z",
          "iopub.execute_input": "2024-07-14T13:27:58.307314Z",
          "iopub.status.idle": "2024-07-14T13:27:58.322219Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.307268Z",
          "shell.execute_reply": "2024-07-14T13:27:58.321143Z"
        },
        "trusted": true,
        "id": "oKULOH3_A2k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Using features: NO2_total, NO2_trop, {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "def feature_combination_analysis(train, target_col, groups, n_splits):\n",
        "    base_features = ['NO2_total', 'NO2_trop']\n",
        "    additional_features = [col for col in train.drop(columns=target_col).columns if col not in base_features]\n",
        "\n",
        "    feature_importances = {}\n",
        "    for col in additional_features:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train[[target_col] + base_features + [col]], train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v[[target_col] + base_features + [col]], test_v[[target_col] + base_features + [col]], target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} with base features, RMSE: {feature_rmse}')\n",
        "    sorted_features = sorted(feature_importances.items(), key=lambda x: x[1])\n",
        "\n",
        "    print('Feature importances:')\n",
        "    for feature, importance in sorted_features:\n",
        "        print(f'{feature}: {importance}')\n",
        "\n",
        "    return feature_importances\n",
        "\n",
        "feature_importances = feature_combination_analysis(train, 'GT_NO2', groups, n_splits)\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.323495Z",
          "iopub.execute_input": "2024-07-14T13:27:58.323834Z",
          "iopub.status.idle": "2024-07-14T13:27:58.336636Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.323805Z",
          "shell.execute_reply": "2024-07-14T13:27:58.335419Z"
        },
        "trusted": true,
        "id": "3RdbdtLXA2k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Using feature: {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "\n",
        "def single_feature_analysis(train, target_col, groups, n_splits):\n",
        "    feature_importances = {}\n",
        "    for col in train.drop(columns=target_col).columns:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train[[target_col, col]], train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v[[target_col, col]], test_v[[target_col, col]], target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} only, RMSE: {feature_rmse}')\n",
        "    sorted_features = sorted(feature_importances.items(), key=lambda x: x[1])\n",
        "\n",
        "    print('Feature importances:')\n",
        "    for feature, importance in sorted_features:\n",
        "        print(f'{feature}: {importance}')\n",
        "\n",
        "    return feature_importances\n",
        "feature_importances = single_feature_analysis(train, 'GT_NO2', groups, n_splits)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.338904Z",
          "iopub.execute_input": "2024-07-14T13:27:58.339441Z",
          "iopub.status.idle": "2024-07-14T13:27:58.353303Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.3394Z",
          "shell.execute_reply": "2024-07-14T13:27:58.352146Z"
        },
        "trusted": true,
        "id": "iaxeV2MtA2k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hshsd"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.354742Z",
          "iopub.execute_input": "2024-07-14T13:27:58.355125Z",
          "iopub.status.idle": "2024-07-14T13:27:58.388466Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.355094Z",
          "shell.execute_reply": "2024-07-14T13:27:58.38673Z"
        },
        "trusted": true,
        "id": "cu-5huLAA2k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train.drop(columns='GT_NO2'),train['GT_NO2'])\n",
        "y = model.predict(test)\n",
        "smaple = pd.read_csv('/Users/ahmed/Downloads/computerscience/Kaggle/geoai-ground-level-no2-estimation/geoai-ground/data/SampleSubmission.csv')\n",
        "smaple['GT_NO2'] = post_process(test,y)\n",
        "smaple.to_csv('submissionpost2.csv',index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.389612Z",
          "iopub.status.idle": "2024-07-14T13:27:58.390052Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.389868Z",
          "shell.execute_reply": "2024-07-14T13:27:58.389886Z"
        },
        "trusted": true,
        "id": "fqBHloMqA2k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train.drop(columns='GT_NO2'),train['GT_NO2'])\n",
        "y_pred = model.predict(test)\n",
        "sub_df = pd.DataFrame({'id': test_id,'GT_NO2':y_pred})\n",
        "sub_df.to_csv('submission10016482406802602.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:32:00.516715Z",
          "iopub.execute_input": "2024-07-14T13:32:00.517132Z",
          "iopub.status.idle": "2024-07-14T13:32:11.521785Z",
          "shell.execute_reply.started": "2024-07-14T13:32:00.517099Z",
          "shell.execute_reply": "2024-07-14T13:32:11.52073Z"
        },
        "trusted": true,
        "id": "cp0d4T3DA2k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = model.feature_importances_\n",
        "names = model.feature_names_\n",
        "fi = pd.DataFrame({'Feature': names,\n",
        "                   'importances': importances})\n",
        "fi = fi.sort_values(by='importances', ascending=False)\n",
        "\n",
        "fi.plot(kind='bar', x='Feature', y='importances', legend=False, figsize=(10, 6))\n",
        "plt.title('Feature Importances')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:33:07.383329Z",
          "iopub.execute_input": "2024-07-14T13:33:07.383755Z",
          "iopub.status.idle": "2024-07-14T13:33:07.689903Z",
          "shell.execute_reply.started": "2024-07-14T13:33:07.383723Z",
          "shell.execute_reply": "2024-07-14T13:33:07.688533Z"
        },
        "trusted": true,
        "id": "9m6QE2TgA2k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:33:32.86811Z",
          "iopub.execute_input": "2024-07-14T13:33:32.869208Z",
          "iopub.status.idle": "2024-07-14T13:33:32.876484Z",
          "shell.execute_reply.started": "2024-07-14T13:33:32.869166Z",
          "shell.execute_reply": "2024-07-14T13:33:32.875389Z"
        },
        "trusted": true,
        "id": "NLSlX2mtA2k-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}