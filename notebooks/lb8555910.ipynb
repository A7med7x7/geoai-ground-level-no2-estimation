{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8770223,
          "sourceType": "datasetVersion",
          "datasetId": 5270243
        }
      ],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'zindi-geoai-ground-level-no2-estimation-challenge:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5270243%2F8770223%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240728%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240728T072246Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D89b854839443b381b3d760b390ae85a96be233d87b53f397f36b2704de23e9dc5c0906315a80faa3e0806daba35e5d63abc2e968c043e1e21ab21b75555b0001c122949fa9f5d5b76700eb66807ea02bb94feb040c9ebfbdfe7202b3d6fa3872c362035a423b6d58c5ae0bf90c27a70ed2578ffd728a7f35ee6c9962bcb06479e0e17b478f60497f8dddf4361a91227fbca93955e91f9797677d9e566f621fc7130e639d2254afccb43ec6ed5f6187af834be2ef887748e33dc6119a3ff6eca4736e89e73ecfefcbe26e5e7dd5f4da6fb740e816a6db7985a4b99d460377fe89cb3fffaa55072fae124bfe914c6729f0fac08100f71cd14c3724c1abb58e94d2'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "863HHJcDA2kz",
        "outputId": "e11ce288-8ce7-4dac-81c8-c8ca1cb200f6"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading zindi-geoai-ground-level-no2-estimation-challenge, 3366877 bytes compressed\n",
            "[==================================================] 3366877 bytes downloaded\n",
            "Downloaded and uncompressed: zindi-geoai-ground-level-no2-estimation-challenge\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importings"
      ],
      "metadata": {
        "id": "GbxChli6A2k1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- mutual information\n",
        "- search for the catboost regressor on kaggle"
      ],
      "metadata": {
        "id": "86s-4DVxA2k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd                                    # for data\n",
        "import numpy as np                                     # for math\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error         # Regressortion metric\n",
        "from sklearn.model_selection import GroupKFold,KFold, TimeSeriesSplit   # for validation\n",
        "from sklearn.preprocessing import LabelEncoder         # for encoding\n",
        "import sklearn.manifold._t_sne as tsne                 # for t_sne\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import TimeSeriesSplit# for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from matplotlib import rcParams\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from scipy.stats import rankdata\n",
        "import xgboost as xgb\n",
        "from sklearn.cluster import KMeans\n",
        "from path import Path\n",
        "path = Path('/kaggle/input/zindi-geoai-ground-level-no2-estimation-challenge')\n",
        "train = pd.read_csv(path /'Train.csv')\n",
        "test = pd.read_csv(path /'Test.csv')\n",
        "groups = train['ID']\n",
        "test_id = test['ID_Zindi']\n",
        "pd.options.display.max_columns = 200\n",
        "#train = train.dropna(axis=0)\n",
        "#test = test.dropna(axis=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:07.741877Z",
          "iopub.execute_input": "2024-07-14T13:27:07.74247Z",
          "iopub.status.idle": "2024-07-14T13:27:07.982266Z",
          "shell.execute_reply.started": "2024-07-14T13:27:07.742426Z",
          "shell.execute_reply": "2024-07-14T13:27:07.980998Z"
        },
        "trusted": true,
        "id": "wfN_LZD2A2k2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train = train[train['Precipitation'] < 112.000]"
      ],
      "metadata": {
        "id": "fbZpGujEawaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.dropna(subset=['GT_NO2'])\n",
        "train.isnull().sum()\n",
        "#in order to the catboost model to be evaluated"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:07.984154Z",
          "iopub.execute_input": "2024-07-14T13:27:07.984596Z",
          "iopub.status.idle": "2024-07-14T13:27:08.02151Z",
          "shell.execute_reply.started": "2024-07-14T13:27:07.984557Z",
          "shell.execute_reply": "2024-07-14T13:27:08.020297Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg8w61uzA2k3",
        "outputId": "a80a0781-2791-40a3-a238-b542d20e6293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID_Zindi                  0\n",
              "Date                      0\n",
              "ID                        0\n",
              "LAT                       0\n",
              "LON                       0\n",
              "Precipitation             0\n",
              "LST                   37594\n",
              "AAI                   12118\n",
              "CloudFraction         12118\n",
              "NO2_strat             12118\n",
              "NO2_total             12118\n",
              "NO2_trop              33429\n",
              "TropopausePressure    12118\n",
              "GT_NO2                    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "PTg6C2AgA2k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_feats = train.select_dtypes(include=['float'])\n",
        "kmeans = KMeans(n_clusters=2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:08.032298Z",
          "iopub.execute_input": "2024-07-14T13:27:08.03265Z",
          "iopub.status.idle": "2024-07-14T13:27:08.040844Z",
          "shell.execute_reply.started": "2024-07-14T13:27:08.03262Z",
          "shell.execute_reply": "2024-07-14T13:27:08.039708Z"
        },
        "trusted": true,
        "id": "dxYxybSVA2k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lat_min, lat_max = 44.92469405, 45.88973369\n",
        "lon_min, lon_max = 8.736496578, 12.59068235\n",
        "\n",
        "num_clusters_lat = 3\n",
        "num_clusters_lon = 4\n",
        "lat_step = (lat_max - lat_min) / num_clusters_lat\n",
        "lon_step = (lon_max - lon_min) / num_clusters_lon\n",
        "def assign_clusters(row, lat_step, lon_step, lat_min, lon_min):\n",
        "    lat_cluster = int((row['LAT'] - lat_min) / lat_step)\n",
        "    lon_cluster = int((row['LON'] - lon_min) / lon_step)\n",
        "    return lat_cluster, lon_cluster\n",
        "for dataset in (train, test):\n",
        "    dataset[['lat_cluster', 'lon_cluster']] = dataset.apply(\n",
        "        assign_clusters, axis=1, result_type='expand',\n",
        "        lat_step=lat_step, lon_step=lon_step, lat_min=lat_min, lon_min=lon_min)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:08.041968Z",
          "iopub.execute_input": "2024-07-14T13:27:08.042411Z",
          "iopub.status.idle": "2024-07-14T13:27:11.331717Z",
          "shell.execute_reply.started": "2024-07-14T13:27:08.042375Z",
          "shell.execute_reply": "2024-07-14T13:27:11.330532Z"
        },
        "trusted": true,
        "id": "Q4vR1sbXA2k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"train = train.sort_values('Date').reset_index(drop=True)\n",
        "test = test.sort_values('Date').reset_index(drop=True)\n",
        "\"\"\"\n",
        "for df in (train, test):\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "for df in (train, test):\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Month_Day'] = df['Month'].astype(str) + '-' + df['Day'].astype(str)\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Weekday'] = df['Date'].dt.weekday\n",
        "    df['Year_Week'] = df['Year'].astype(str) + '-' + df['Weekday'].astype(str)\n",
        "    df.drop(columns=['Weekday', 'Year', 'Day', 'ID_Zindi'], inplace=True)\n",
        "\n",
        "# Linterations\n",
        "for df in(train,test):\n",
        "    df['Month_lag1'] = df['Month'].shift(1)\n",
        "    df['Month_lag2'] = df['Month'].shift(2)\n",
        "    #df['Month_lag-1'] = df['Month'].shift(-1)\n",
        "\n",
        "    #df['Month_lag3'] = df['Month'].shift(3)\n",
        "    #df['MonthDay_lag1'] = df['Month_Day'].shift(1)\n",
        "    #df['NO2_strat_lag_1'] = df['NO2_strat'].shift(1)\n",
        "    #df['NO2_strat_lag_7'] = df['NO2_strat'].shift(7)\n",
        "    #df['Precipitation_inter1'] = df['TropopausePressure'] + df['Precipitation']\n",
        "    #df['Precipitation_log'] =  np.log(df['Precipitation'] + 1e-9)\n",
        "    #df['CloudFraction_diff'] = df['CloudFraction'] / df['NO2_strat']\n",
        "    #df['CloudFraction_diff2'] = df['CloudFraction'] / df['NO2_total']\n",
        "    #['Precipitation_fractional'] = df['Precipitation'] * 0.00001\n",
        "    #df['TropopausePressure_fractional'] = round(df['TropopausePressure'] * 0.00001,2)\n",
        "    #df['cluster'] = kmeans.fit_transform(df[['LAT', 'LON']])\n",
        "    #df['cloud_fraction1'] = df['CloudFraction'] % df['NO2_strat']\n",
        "    #df['cloud_fraction2'] = df['CloudFraction'] % df['NO2_total']\n",
        "\n",
        "#for df in(train,test):\n",
        "    #Rolling (Moving Average)\n",
        "    #df['feature_rolling_3_mean'] = df['TropopausePressure'].rolling(5).mean()\n",
        "    #df['feature_rolling_7_mean'] = df['TropopausePressure'].rolling(7).mean()\n",
        "    #df['feature_rolling_7_std'] = df['TropopausePressure'].rolling(7).std()\n",
        "    #df['NO2_strat_rolling7'] = df['NO2_strat'].rolling(7).mean()\n",
        "    #df['NO2_strat_rolling30'] = df['NO2_strat'].rolling(30).mean()\n",
        "    #df['NO2_strat_rolling9'] = df['NO2_strat'].rolling(9).mean()\n",
        "#statsitics of similar variables"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:11.360855Z",
          "iopub.execute_input": "2024-07-14T13:27:11.361234Z",
          "iopub.status.idle": "2024-07-14T13:27:11.529134Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.361203Z",
          "shell.execute_reply": "2024-07-14T13:27:11.527859Z"
        },
        "trusted": true,
        "id": "w-UjVVwUA2k5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94623991-7b69-4820-a01f-bc0a21a9235c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-199-53f3be32645b>:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n",
            "<ipython-input-199-53f3be32645b>:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# GT_NO2_per_day, NO2_strat_per_day, avg_GT_NO2_per_customer and GT_NO2_per_NO2_strat_per_day\n",
        "\n",
        "# Get total GT_NO2, NO2_strat and open days per lon_cluster\n",
        "lon_cluster_data_GT_NO2 = train.groupby([train['lon_cluster']])['GT_NO2'].sum()\n",
        "lon_cluster_data_NO2_strat = train.groupby([train['lon_cluster']])['NO2_strat'].sum()\n",
        "lon_cluster_data_avg_GT_NO2 = train.groupby([train['lon_cluster']])['GT_NO2'].mean()\n",
        "lon_cluster_data_avg_NO2_strat = train.groupby([train['lon_cluster']])['NO2_strat'].mean()\n",
        "lon_cluster_data_open = train.groupby([train['lon_cluster']])['NO2_total'].count()\n",
        "\n",
        "# Calculate GT_NO2 per day, NO2_strat per day and GT_NO2 per NO2_strat per day\n",
        "lon_cluster_data_GT_NO2_per_day = lon_cluster_data_GT_NO2 / lon_cluster_data_open\n",
        "lon_cluster_data_NO2_strat_per_day = lon_cluster_data_NO2_strat / lon_cluster_data_open\n",
        "lon_cluster_data_avg_GT_NO2_per_customer = lon_cluster_data_avg_GT_NO2 / lon_cluster_data_avg_NO2_strat\n",
        "lon_cluster_data_GT_NO2_per_customer_per_day = lon_cluster_data_GT_NO2_per_day / lon_cluster_data_NO2_strat_per_day\n",
        "\n",
        "#Saving the above values in a dictionary so that they can be mapped to the dataframe.\n",
        "GT_NO2_per_day_dict = dict(lon_cluster_data_GT_NO2_per_day)\n",
        "NO2_strat_per_day_dict = dict(lon_cluster_data_NO2_strat_per_day)\n",
        "avg_GT_NO2_per_customer_dict = dict(lon_cluster_data_avg_GT_NO2_per_customer)\n",
        "GT_NO2_per_NO2_strat_per_day_dict = dict(lon_cluster_data_GT_NO2_per_customer_per_day)\n",
        "\n",
        "\n",
        "\n",
        "train['GT_NO2PerDay'] = train['lon_cluster'].map(GT_NO2_per_day_dict)\n",
        "#train['NO2_strat_per_day'] = train['lon_cluster'].map(NO2_strat_per_day_dict)\n",
        "train['Avg_GT_NO2_per_Customer'] = train['lon_cluster'].map(avg_GT_NO2_per_customer_dict)\n",
        "#train['GT_NO2_Per_NO2_strat_Per_Day'] = train['lon_cluster'].map(GT_NO2_per_NO2_strat_per_day_dict)\n",
        "\n",
        "test['GT_NO2PerDay'] = test['lon_cluster'].map(GT_NO2_per_day_dict)\n",
        "#test['NO2_strat_per_day'] = test['lon_cluster'].map(NO2_strat_per_day_dict)\n",
        "test['Avg_GT_NO2_per_Customer'] = test['lon_cluster'].map(avg_GT_NO2_per_customer_dict)\n",
        "#test['GT_NO2_Per_NO2_strat_Per_Day'] = test['lon_cluster'].map(GT_NO2_per_NO2_strat_per_day_dict)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "uhZ45CPqZ2h3",
        "outputId": "d5736665-60f7-43f3-c4ce-74a3e0d3bfcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# GT_NO2_per_day, NO2_strat_per_day, avg_GT_NO2_per_customer and GT_NO2_per_NO2_strat_per_day\\n\\n# Get total GT_NO2, NO2_strat and open days per lon_cluster\\nlon_cluster_data_GT_NO2 = train.groupby([train['lon_cluster']])['GT_NO2'].sum()\\nlon_cluster_data_NO2_strat = train.groupby([train['lon_cluster']])['NO2_strat'].sum()\\nlon_cluster_data_avg_GT_NO2 = train.groupby([train['lon_cluster']])['GT_NO2'].mean()\\nlon_cluster_data_avg_NO2_strat = train.groupby([train['lon_cluster']])['NO2_strat'].mean()\\nlon_cluster_data_open = train.groupby([train['lon_cluster']])['NO2_total'].count()\\n\\n# Calculate GT_NO2 per day, NO2_strat per day and GT_NO2 per NO2_strat per day\\nlon_cluster_data_GT_NO2_per_day = lon_cluster_data_GT_NO2 / lon_cluster_data_open\\nlon_cluster_data_NO2_strat_per_day = lon_cluster_data_NO2_strat / lon_cluster_data_open\\nlon_cluster_data_avg_GT_NO2_per_customer = lon_cluster_data_avg_GT_NO2 / lon_cluster_data_avg_NO2_strat\\nlon_cluster_data_GT_NO2_per_customer_per_day = lon_cluster_data_GT_NO2_per_day / lon_cluster_data_NO2_strat_per_day\\n\\n#Saving the above values in a dictionary so that they can be mapped to the dataframe.\\nGT_NO2_per_day_dict = dict(lon_cluster_data_GT_NO2_per_day)\\nNO2_strat_per_day_dict = dict(lon_cluster_data_NO2_strat_per_day)\\navg_GT_NO2_per_customer_dict = dict(lon_cluster_data_avg_GT_NO2_per_customer)\\nGT_NO2_per_NO2_strat_per_day_dict = dict(lon_cluster_data_GT_NO2_per_customer_per_day)\\n\\n\\n\\ntrain['GT_NO2PerDay'] = train['lon_cluster'].map(GT_NO2_per_day_dict)\\n#train['NO2_strat_per_day'] = train['lon_cluster'].map(NO2_strat_per_day_dict)\\ntrain['Avg_GT_NO2_per_Customer'] = train['lon_cluster'].map(avg_GT_NO2_per_customer_dict)\\n#train['GT_NO2_Per_NO2_strat_Per_Day'] = train['lon_cluster'].map(GT_NO2_per_NO2_strat_per_day_dict)\\n\\ntest['GT_NO2PerDay'] = test['lon_cluster'].map(GT_NO2_per_day_dict)\\n#test['NO2_strat_per_day'] = test['lon_cluster'].map(NO2_strat_per_day_dict)\\ntest['Avg_GT_NO2_per_Customer'] = test['lon_cluster'].map(avg_GT_NO2_per_customer_dict)\\n#test['GT_NO2_Per_NO2_strat_Per_Day'] = test['lon_cluster'].map(GT_NO2_per_NO2_strat_per_day_dict)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def MeanSd(feature1, feature2):\n",
        "    for dataset in (train,test):\n",
        "        dataset[\"SD\" + feature1] = dataset[[feature1,feature2]].std(axis=1)\n",
        "        dataset[\"MEAN\" + feature1] = dataset[[feature1,feature2]].mean(axis=1)\n",
        "\n",
        "MeanSd('NO2_total','NO2_strat')\n",
        "MeanSd('LON','LAT')\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "y8DWD6OAF-ud",
        "outputId": "f7b053ed-36e0-4f31-96a2-c53c18064123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef MeanSd(feature1, feature2):\\n    for dataset in (train,test):\\n        dataset[\"SD\" + feature1] = dataset[[feature1,feature2]].std(axis=1)\\n        dataset[\"MEAN\" + feature1] = dataset[[feature1,feature2]].mean(axis=1)\\n\\nMeanSd(\\'NO2_total\\',\\'NO2_strat\\')\\nMeanSd(\\'LON\\',\\'LAT\\')'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Make sure to fill NA values if any due to rolling and shifting"
      ],
      "metadata": {
        "id": "lx9eK7IXquxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for df in(train,test):\n",
        "    #df['LST_mean_60'] = df['LST'].rolling(60).mean()\n",
        "    #df['Prec_mean_60'] = df['Precipitation'].rolling(60).mean()\n",
        "    #df['AII_mean'] = df['AAI'].rolling(60).mean()\n",
        "    #df['no2_total'] = df['NO2_total'].rolling(60).min()\n",
        "\n",
        "    #df['feature_rolling_3_mean'] = df['TropopausePressure'].rolling(5).mean()\n",
        "    #df['feature_rolling_7_mean'] = df['TropopausePressure'].rolling(7).mean()\n",
        "    #df['feature_rolling_7_std'] = df['TropopausePressure'].rolling(7).std()\n",
        "    #df['NO2_strat_rolling7'] = df['NO2_strat'].rolling(7).mean()\n",
        "    #df['NO2_strat_rolling30'] = df['NO2_strat'].rolling(30).mean()\n",
        "    #df['NO2_strat_rolling2'] = df['NO2_strat'].rolling(9).mean()\n",
        "#for df in(train,test):\n",
        "  #df['Precipitation_roll_max30'] = df['Precipitation'].rolling(30).max()\n",
        "  #df['Precipitation_roll_mean30'] = df['Precipitation'].rolling(30).mean()\n",
        "  #df['Precipitation_roll_mean60'] = df['Precipitation'].rolling(60).mean()\n",
        "def rolling(feature):\n",
        "    for dataset in (train,test):\n",
        "        #dataset[f\"{feature}_rolling_mean_60\"] = dataset[feature].rolling(60).mean()\n",
        "        dataset[f\"{feature}_rolling_max_60\"] = dataset[feature].rolling(60).max()\n",
        "        #dataset[f\"{feature}_rolling_max_30\"] = dataset[feature].rolling(30).max()\n",
        "        #dataset[f\"{feature}_rolling_min_60\"] = dataset[feature].rolling(60).min()\n",
        "\n",
        "rolling('NO2_trop')\n",
        "rolling('NO2_total')\n",
        "rolling('TropopausePressure')\n",
        "rolling('CloudFraction')\n",
        "#rolling('AAI')\n",
        "#rolling('LST')\n",
        "rolling('Precipitation')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:11.530748Z",
          "iopub.execute_input": "2024-07-14T13:27:11.531198Z",
          "iopub.status.idle": "2024-07-14T13:27:11.565633Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.53115Z",
          "shell.execute_reply": "2024-07-14T13:27:11.564625Z"
        },
        "trusted": true,
        "id": "oXVjF7-PA2k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Values & Encoding"
      ],
      "metadata": {
        "id": "I8bwdOoRA2k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groups = train['ID']\n",
        "for df in(train,test):\n",
        "    df.drop(columns=[\"Date\",'ID','Precipitation','CloudFraction','AAI','lat_cluster'], axis=1,inplace=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "for df in(train,test):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = le.fit_transform(df[col])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:11.579374Z",
          "iopub.execute_input": "2024-07-14T13:27:11.579757Z",
          "iopub.status.idle": "2024-07-14T13:27:11.597165Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.579716Z",
          "shell.execute_reply": "2024-07-14T13:27:11.596076Z"
        },
        "trusted": true,
        "id": "9Q7DTxHNA2k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CV and Modeling"
      ],
      "metadata": {
        "id": "KELbHAFzA2k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns # ['LAT', 'LON', 'LST', 'NO2_strat', 'NO2_total', 'NO2_trop', 'TropopausePressure', 'GT_NO2', 'lon_cluster', 'Month', 'Month_Day', 'Year_Week', 'Month_lag1', 'Month_lag2', 'NO2_trop_rolling_max_60', 'NO2_total_rolling_max_60', 'TropopausePressure_rolling_max_60', 'CloudFraction_rolling_max_60', 'Precipitation_rolling_max_60'],"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:11.644483Z",
          "iopub.execute_input": "2024-07-14T13:27:11.644891Z",
          "iopub.status.idle": "2024-07-14T13:27:11.660164Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.644856Z",
          "shell.execute_reply": "2024-07-14T13:27:11.65905Z"
        },
        "trusted": true,
        "id": "cpuC0D5VA2k8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9fe7a7-fe25-4c33-f805-990aa0fd199c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['LAT', 'LON', 'LST', 'NO2_strat', 'NO2_total', 'NO2_trop',\n",
              "       'TropopausePressure', 'GT_NO2', 'lon_cluster', 'Month', 'Month_Day',\n",
              "       'Year_Week', 'Month_lag1', 'Month_lag2', 'NO2_trop_rolling_max_60',\n",
              "       'NO2_total_rolling_max_60', 'TropopausePressure_rolling_max_60',\n",
              "       'CloudFraction_rolling_max_60', 'Precipitation_rolling_max_60'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model =  LGBMRegressor(random_state=7)\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRFRegressor\n",
        "#model = XGBRFRegressor(random_state=7)\n",
        "model = CatBoostRegressor(\n",
        "    iterations=1000,        # Number of boosting iterations\n",
        "    learning_rate=0.1,      # Learning rate\n",
        "    depth=6,                # Depth of the tree\n",
        "    loss_function='RMSE',   # Loss function\n",
        "    eval_metric='RMSE',     # Evaluation metric\n",
        "    random_seed=7,          # Random seed for reproducibility\n",
        "    verbose=100             # Print information every 100 iterations\n",
        ")\n",
        "\n",
        "#model = CatBoostRegressor(random_state=7)\n",
        "#model = XGBRegressor(random_state= 7)\n",
        "n_splits = 5\n",
        "n = train['GT_NO2'].count()\n",
        "num_bins = int(1 + np.log2(n))\n",
        "cv = GroupKFold(n_splits=n_splits)\n",
        "\n",
        "def validate(trainset, testset, target_col):\n",
        "\n",
        "    model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    print('std:', valid_testset.std())\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    print('score:', score)\n",
        "\n",
        "    return score\n",
        "stds = []\n",
        "rmse = []\n",
        "\n",
        "for train_idx, test_idx in cv.split(train.drop(columns='GT_NO2'), train['GT_NO2'], groups=groups):\n",
        "    train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "    stds.append(test_v['GT_NO2'].std())\n",
        "    rmse.append(validate(train_v, test_v, 'GT_NO2'))\n",
        "\n",
        "print('RMSE:', np.array(rmse).mean())\n",
        "print('RMSE std:', np.array(rmse).std())\n",
        "print('Standard Deviations:', stds)\n",
        "print('RMSEs Deviations:', rmse)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:11.661856Z",
          "iopub.execute_input": "2024-07-14T13:27:11.662298Z",
          "iopub.status.idle": "2024-07-14T13:27:58.266503Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.662257Z",
          "shell.execute_reply": "2024-07-14T13:27:58.264922Z"
        },
        "trusted": true,
        "id": "HHaRM8N7A2k8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1d9a3b-a757-46ae-ff0c-9891d42e6cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 16.7613911\ttotal: 24.2ms\tremaining: 24.2s\n",
            "100:\tlearn: 9.1952985\ttotal: 2.51s\tremaining: 22.3s\n",
            "200:\tlearn: 8.4761942\ttotal: 4.94s\tremaining: 19.6s\n",
            "300:\tlearn: 8.0383997\ttotal: 6.33s\tremaining: 14.7s\n",
            "400:\tlearn: 7.7164032\ttotal: 7.71s\tremaining: 11.5s\n",
            "500:\tlearn: 7.4738979\ttotal: 9.09s\tremaining: 9.05s\n",
            "600:\tlearn: 7.2674287\ttotal: 10.5s\tremaining: 6.96s\n",
            "700:\tlearn: 7.0885328\ttotal: 11.8s\tremaining: 5.05s\n",
            "800:\tlearn: 6.9349250\ttotal: 13.2s\tremaining: 3.29s\n",
            "900:\tlearn: 6.8020462\ttotal: 15.2s\tremaining: 1.67s\n",
            "999:\tlearn: 6.6749377\ttotal: 18.1s\tremaining: 0us\n",
            "std: 14.76802799749277\n",
            "score: 9.73739825412034\n",
            "0:\tlearn: 15.5641502\ttotal: 52.9ms\tremaining: 52.8s\n",
            "100:\tlearn: 8.7591437\ttotal: 2.67s\tremaining: 23.7s\n",
            "200:\tlearn: 8.0796790\ttotal: 4.88s\tremaining: 19.4s\n",
            "300:\tlearn: 7.6831306\ttotal: 6.4s\tremaining: 14.9s\n",
            "400:\tlearn: 7.3891856\ttotal: 7.8s\tremaining: 11.7s\n",
            "500:\tlearn: 7.1586526\ttotal: 9.17s\tremaining: 9.14s\n",
            "600:\tlearn: 6.9701573\ttotal: 10.6s\tremaining: 7.01s\n",
            "700:\tlearn: 6.8200355\ttotal: 13.6s\tremaining: 5.82s\n",
            "800:\tlearn: 6.6852058\ttotal: 15.3s\tremaining: 3.81s\n",
            "900:\tlearn: 6.5651028\ttotal: 16.7s\tremaining: 1.83s\n",
            "999:\tlearn: 6.4627907\ttotal: 18s\tremaining: 0us\n",
            "std: 19.317676241120203\n",
            "score: 10.41188682776457\n",
            "0:\tlearn: 16.0230100\ttotal: 14.6ms\tremaining: 14.6s\n",
            "100:\tlearn: 9.0363256\ttotal: 1.39s\tremaining: 12.4s\n",
            "200:\tlearn: 8.3371374\ttotal: 2.8s\tremaining: 11.1s\n",
            "300:\tlearn: 7.9135667\ttotal: 4.2s\tremaining: 9.75s\n",
            "400:\tlearn: 7.6191415\ttotal: 5.57s\tremaining: 8.32s\n",
            "500:\tlearn: 7.3868555\ttotal: 8.11s\tremaining: 8.08s\n",
            "600:\tlearn: 7.1945017\ttotal: 10.2s\tremaining: 6.79s\n",
            "700:\tlearn: 7.0337008\ttotal: 11.6s\tremaining: 4.94s\n",
            "800:\tlearn: 6.8935819\ttotal: 13s\tremaining: 3.22s\n",
            "900:\tlearn: 6.7592265\ttotal: 14.4s\tremaining: 1.58s\n",
            "999:\tlearn: 6.6421539\ttotal: 15.7s\tremaining: 0us\n",
            "std: 18.309333259946925\n",
            "score: 9.67846986781879\n",
            "0:\tlearn: 16.5826782\ttotal: 15ms\tremaining: 15s\n",
            "100:\tlearn: 9.1956700\ttotal: 1.41s\tremaining: 12.5s\n",
            "200:\tlearn: 8.4986469\ttotal: 2.79s\tremaining: 11.1s\n",
            "300:\tlearn: 8.0798626\ttotal: 5s\tremaining: 11.6s\n",
            "400:\tlearn: 7.7692467\ttotal: 7.45s\tremaining: 11.1s\n",
            "500:\tlearn: 7.5344770\ttotal: 8.86s\tremaining: 8.82s\n",
            "600:\tlearn: 7.3388423\ttotal: 10.3s\tremaining: 6.82s\n",
            "700:\tlearn: 7.1611271\ttotal: 11.6s\tremaining: 4.96s\n",
            "800:\tlearn: 7.0135278\ttotal: 13s\tremaining: 3.23s\n",
            "900:\tlearn: 6.8758043\ttotal: 14.4s\tremaining: 1.58s\n",
            "999:\tlearn: 6.7619524\ttotal: 15.8s\tremaining: 0us\n",
            "std: 15.740500667738324\n",
            "score: 9.186038810785229\n",
            "0:\tlearn: 16.4729168\ttotal: 14.2ms\tremaining: 14.2s\n",
            "100:\tlearn: 9.1702453\ttotal: 2.09s\tremaining: 18.6s\n",
            "200:\tlearn: 8.4489832\ttotal: 4.7s\tremaining: 18.7s\n",
            "300:\tlearn: 8.0175333\ttotal: 6.11s\tremaining: 14.2s\n",
            "400:\tlearn: 7.7129593\ttotal: 7.48s\tremaining: 11.2s\n",
            "500:\tlearn: 7.4751766\ttotal: 8.89s\tremaining: 8.86s\n",
            "600:\tlearn: 7.2793154\ttotal: 10.3s\tremaining: 6.83s\n",
            "700:\tlearn: 7.1068327\ttotal: 11.7s\tremaining: 4.97s\n",
            "800:\tlearn: 6.9560125\ttotal: 13.1s\tremaining: 3.25s\n",
            "900:\tlearn: 6.8162486\ttotal: 14.7s\tremaining: 1.62s\n",
            "999:\tlearn: 6.6992585\ttotal: 17.5s\tremaining: 0us\n",
            "std: 16.332857822117944\n",
            "score: 8.492401028481611\n",
            "RMSE: 9.501238957794108\n",
            "RMSE std: 0.6377408019070387\n",
            "Standard Deviations: [14.76802799749277, 19.317676241120203, 18.309333259946925, 15.740500667738324, 16.332857822117944]\n",
            "RMSEs Deviations: [9.73739825412034, 10.41188682776457, 9.67846986781879, 9.186038810785229, 8.492401028481611]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9.83178851079467 the month day feature added\n",
        "#9.666401979691653 best of lb = 8.592019681\n",
        "#9.658790989115184 adding params to the model - setting objective"
      ],
      "metadata": {
        "id": "plzHI6cnKA3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "metadata": {
        "id": "0iJ5kLVxnAgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23923c3-2b69-4f65-b64c-367c7adea4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['LAT', 'LON', 'LST', 'NO2_strat', 'NO2_total', 'NO2_trop',\n",
              "       'TropopausePressure', 'GT_NO2', 'lon_cluster', 'Month', 'Month_Day',\n",
              "       'Year_Week', 'Month_lag1', 'Month_lag2', 'NO2_trop_rolling_max_60',\n",
              "       'NO2_total_rolling_max_60', 'TropopausePressure_rolling_max_60',\n",
              "       'CloudFraction_rolling_max_60', 'Precipitation_rolling_max_60'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10.155311364148783 catboost leaderboard best s far 9.525616244\n",
        "#9.920018431939276 catboost and added the day features - probably it overfits - indeed\n",
        "#9.43514150859055 removing all missing values leaderboard 11.9315086\n",
        "#10.139203906659699\n",
        "#10.201862135497844 the effect of mutual information\n",
        "#9.476601295728365\n",
        "#10.025125392453269"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.268096Z",
          "iopub.execute_input": "2024-07-14T13:27:58.268549Z",
          "iopub.status.idle": "2024-07-14T13:27:58.273759Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.268508Z",
          "shell.execute_reply": "2024-07-14T13:27:58.272513Z"
        },
        "trusted": true,
        "id": "v_2Fi9rJA2k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11.581549939504997 with month ,\n",
        "#12.246931687286574\n",
        "#12.235259341356834\n",
        "#12.231869616518921\n",
        "#12.126333476852636\n",
        "#11.728753513719479\n",
        "#11.724263538742687\n",
        "#11.728753513719479\n",
        "#11.711277895239192\n",
        "#11.694341231865433\n",
        "#11.671982426722977\n",
        "#11.642227859275334 without the mean - with the custamized means of rolling\n",
        "#11.314985626750522\n",
        "#11.314985626750522\n",
        "#11.312413659993586\n",
        "#11.135932399851725\n",
        "#11.728753513719479 best so far features = ['LAT', 'Month','NO2_trop_rolling_max_60','NO2_total_rolling_max_60','TropopausePressure_rolling_max_60','CloudFraction_rolling_max_60','Precipitation_rolling_max_60']\n",
        "#11.40320255907346 with the ID LB = 9.571700669\n",
        "#11.597286160951526 the distance feature"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.275324Z",
          "iopub.execute_input": "2024-07-14T13:27:58.275771Z",
          "iopub.status.idle": "2024-07-14T13:27:58.284828Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.275732Z",
          "shell.execute_reply": "2024-07-14T13:27:58.283586Z"
        },
        "trusted": true,
        "id": "FK5Eig4FA2k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib, os,sys\n",
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Using features: Based_features, {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "def feature_combination_analysis(train, target_col, groups, n_splits):\n",
        "    base_features =  ['LAT', 'LON', 'LST', 'NO2_strat', 'NO2_total', 'NO2_trop', 'TropopausePressure', 'lon_cluster', 'Month', 'Month_Day', 'Year_Week', 'Month_lag1', 'Month_lag2', 'NO2_trop_rolling_max_60', 'NO2_total_rolling_max_60', 'TropopausePressure_rolling_max_60', 'CloudFraction_rolling_max_60', 'Precipitation_rolling_max_60']\n",
        "    additional_features = [col for col in train.drop(columns=target_col).columns if col not in base_features]\n",
        "\n",
        "    feature_importances = {}\n",
        "    for col in additional_features:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train[[target_col] + base_features + [col]], train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v[[target_col] + base_features + [col]], test_v[[target_col] + base_features + [col]], target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} with base features, RMSE: {feature_rmse}')\n",
        "    sorted_features = sorted(feature_importances.items(), key=lambda x: x[1])\n",
        "\n",
        "    print('Feature importances:')\n",
        "    for feature, importance in sorted_features:\n",
        "        print(f'{feature}: {importance}')\n",
        "\n",
        "    return feature_importances\n",
        "feature_importances = feature_combination_analysis(train, 'GT_NO2', groups, n_splits)\n",
        "#9.6664019796\n",
        "##9.666401979691653\n",
        "#9.6587909891 best so far"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.323495Z",
          "iopub.execute_input": "2024-07-14T13:27:58.323834Z",
          "iopub.status.idle": "2024-07-14T13:27:58.336636Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.323805Z",
          "shell.execute_reply": "2024-07-14T13:27:58.335419Z"
        },
        "trusted": true,
        "id": "3RdbdtLXA2k9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6de125-387b-4bec-b511-eeb847086e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#try the least with this base so far removing the LST\n",
        "#using the best base so far and add the cloud fraction"
      ],
      "metadata": {
        "id": "kzJPQR_QYFkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib, os,sys\n",
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Removed feature: {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "\n",
        "def lofo_analysis(train, target_col, groups, n_splits):\n",
        "    base_rmse = []\n",
        "    for train_idx, test_idx in cv.split(train.drop(columns=target_col), train[target_col], groups=groups):\n",
        "        train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "        base_rmse.append(validate(train_v, test_v, target_col))\n",
        "    base_score = np.array(base_rmse).mean()\n",
        "    print('Base RMSE:', base_score)\n",
        "\n",
        "    feature_importances = {}\n",
        "    for col in train.drop(columns=target_col).columns:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train.drop(columns=[target_col, col]), train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v.drop(columns=col), test_v.drop(columns=col), target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} removed, RMSE: {feature_rmse}')\n",
        "\n",
        "    bad_features = [col for col in feature_importances if feature_importances[col] > base_score]\n",
        "    good_features = [col for col in feature_importances if feature_importances[col] <= base_score]\n",
        "\n",
        "    print('Good features:', good_features)\n",
        "    print('Bad features:', bad_features)\n",
        "\n",
        "    return good_features, bad_features, feature_importances\n",
        "\n",
        "good_features, bad_features, feature_importances = lofo_analysis(train, 'GT_NO2', groups, n_splits)\n",
        "print('Feature importances:')\n",
        "for feature, importance in feature_importances.items():\n",
        "    print(f'{feature}: {importance}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.286156Z",
          "iopub.execute_input": "2024-07-14T13:27:58.286545Z",
          "iopub.status.idle": "2024-07-14T13:27:58.304718Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.286509Z",
          "shell.execute_reply": "2024-07-14T13:27:58.303412Z"
        },
        "trusted": true,
        "id": "eaUv4zbZA2k8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "outputId": "9bded871-228b-4678-d117-7546f404155f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 9.822670850417143\n",
            "Validation MSE: 10.549200003562145\n",
            "Validation MSE: 9.774874516906602\n",
            "Validation MSE: 9.436792438470013\n",
            "Validation MSE: 8.710417136220013\n",
            "Base RMSE: 9.658790989115184\n",
            "Evaluating feature: LAT\n",
            "Removed feature: LAT | Validation MSE: 9.79603608963927\n",
            "Removed feature: LAT | Validation MSE: 12.531935729824285\n",
            "Removed feature: LAT | Validation MSE: 11.164722626417456\n",
            "Removed feature: LAT | Validation MSE: 12.625079257919328\n",
            "Removed feature: LAT | Validation MSE: 9.948275691833745\n",
            "Feature LAT removed, RMSE: 11.213209879126817\n",
            "Evaluating feature: LON\n",
            "Removed feature: LON | Validation MSE: 12.388001830494876\n",
            "Removed feature: LON | Validation MSE: 11.554377358971337\n",
            "Removed feature: LON | Validation MSE: 11.047180487093744\n",
            "Removed feature: LON | Validation MSE: 9.867626166066696\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-212-61fbd6467c6b>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgood_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbad_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mgood_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbad_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlofo_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GT_NO2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature importances:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-212-61fbd6467c6b>\u001b[0m in \u001b[0;36mlofo_analysis\u001b[0;34m(train, target_col, groups, n_splits)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mtrain_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mfeature_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mfeature_importances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_rmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-212-61fbd6467c6b>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(trainset, testset, target_col, feature_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msuppress_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvalid_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5825\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5827\u001b[0;31m         return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[1;32m   5828\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5829\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2400\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2401\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.306385Z",
          "iopub.execute_input": "2024-07-14T13:27:58.307314Z",
          "iopub.status.idle": "2024-07-14T13:27:58.322219Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.307268Z",
          "shell.execute_reply": "2024-07-14T13:27:58.321143Z"
        },
        "trusted": true,
        "id": "oKULOH3_A2k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Using feature: {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "\n",
        "def single_feature_analysis(train, target_col, groups, n_splits):\n",
        "    feature_importances = {}\n",
        "    for col in train.drop(columns=target_col).columns:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train[[target_col, col]], train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v[[target_col, col]], test_v[[target_col, col]], target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} only, RMSE: {feature_rmse}')\n",
        "    sorted_features = sorted(feature_importances.items(), key=lambda x: x[1])\n",
        "\n",
        "    print('Feature importances:')\n",
        "    for feature, importance in sorted_features:\n",
        "        print(f'{feature}: {importance}')\n",
        "\n",
        "    return feature_importances\n",
        "feature_importances = single_feature_analysis(train, 'GT_NO2', groups, n_splits)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.338904Z",
          "iopub.execute_input": "2024-07-14T13:27:58.339441Z",
          "iopub.status.idle": "2024-07-14T13:27:58.353303Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.3394Z",
          "shell.execute_reply": "2024-07-14T13:27:58.352146Z"
        },
        "trusted": true,
        "id": "iaxeV2MtA2k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hshsd"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.354742Z",
          "iopub.execute_input": "2024-07-14T13:27:58.355125Z",
          "iopub.status.idle": "2024-07-14T13:27:58.388466Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.355094Z",
          "shell.execute_reply": "2024-07-14T13:27:58.38673Z"
        },
        "trusted": true,
        "id": "cu-5huLAA2k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train.drop(columns='GT_NO2'),train['GT_NO2'])\n",
        "y_pred = model.predict(test)\n",
        "sub_df = pd.DataFrame({'id': test_id,'GT_NO2':y_pred})\n",
        "sub_df.to_csv('submission9.501238957794108.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:32:00.516715Z",
          "iopub.execute_input": "2024-07-14T13:32:00.517132Z",
          "iopub.status.idle": "2024-07-14T13:32:11.521785Z",
          "shell.execute_reply.started": "2024-07-14T13:32:00.517099Z",
          "shell.execute_reply": "2024-07-14T13:32:11.52073Z"
        },
        "trusted": true,
        "id": "cp0d4T3DA2k-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d50aff3-dde8-470a-cd89-d31523e3efb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 16.2849774\ttotal: 16.9ms\tremaining: 16.9s\n",
            "100:\tlearn: 9.1464658\ttotal: 1.63s\tremaining: 14.5s\n",
            "200:\tlearn: 8.4536670\ttotal: 3.25s\tremaining: 12.9s\n",
            "300:\tlearn: 8.0368747\ttotal: 4.85s\tremaining: 11.3s\n",
            "400:\tlearn: 7.7405344\ttotal: 8.34s\tremaining: 12.5s\n",
            "500:\tlearn: 7.5145917\ttotal: 9.95s\tremaining: 9.91s\n",
            "600:\tlearn: 7.3154125\ttotal: 11.6s\tremaining: 7.67s\n",
            "700:\tlearn: 7.1585565\ttotal: 13.1s\tremaining: 5.59s\n",
            "800:\tlearn: 7.0152630\ttotal: 14.8s\tremaining: 3.67s\n",
            "900:\tlearn: 6.8847082\ttotal: 16.3s\tremaining: 1.79s\n",
            "999:\tlearn: 6.7660719\ttotal: 18s\tremaining: 0us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "importances = model.feature_importances_\n",
        "names = model.feature_names_\n",
        "fi = pd.DataFrame({'Feature': names,\n",
        "                   'importances': importances})\n",
        "fi = fi.sort_values(by='importances', ascending=False)\n",
        "\n",
        "fi.plot(kind='bar', x='Feature', y='importances', legend=False, figsize=(10, 6))\n",
        "plt.title('Feature Importances')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:33:07.383329Z",
          "iopub.execute_input": "2024-07-14T13:33:07.383755Z",
          "iopub.status.idle": "2024-07-14T13:33:07.689903Z",
          "shell.execute_reply.started": "2024-07-14T13:33:07.383723Z",
          "shell.execute_reply": "2024-07-14T13:33:07.688533Z"
        },
        "trusted": true,
        "id": "9m6QE2TgA2k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:33:32.86811Z",
          "iopub.execute_input": "2024-07-14T13:33:32.869208Z",
          "iopub.status.idle": "2024-07-14T13:33:32.876484Z",
          "shell.execute_reply.started": "2024-07-14T13:33:32.869166Z",
          "shell.execute_reply": "2024-07-14T13:33:32.875389Z"
        },
        "trusted": true,
        "id": "NLSlX2mtA2k-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}