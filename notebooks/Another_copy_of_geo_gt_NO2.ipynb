{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbxChli6A2k1"
      },
      "source": [
        "# Importings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86s-4DVxA2k2"
      },
      "source": [
        "- mutual information\n",
        "- search for the catboost regressor on kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:07.74247Z",
          "iopub.status.busy": "2024-07-14T13:27:07.741877Z",
          "iopub.status.idle": "2024-07-14T13:27:07.982266Z",
          "shell.execute_reply": "2024-07-14T13:27:07.980998Z",
          "shell.execute_reply.started": "2024-07-14T13:27:07.742426Z"
        },
        "id": "wfN_LZD2A2k2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd                                    # for data\n",
        "import numpy as np                                     # for math\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error         # Regressortion metric\n",
        "from sklearn.model_selection import GroupKFold,KFold, TimeSeriesSplit   # for validation\n",
        "from sklearn.preprocessing import LabelEncoder         # for encoding\n",
        "import sklearn.manifold._t_sne as tsne                 # for t_sne\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import TimeSeriesSplit# for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from matplotlib import rcParams\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from scipy.stats import rankdata\n",
        "import xgboost as xgb\n",
        "from sklearn.cluster import KMeans\n",
        "train = pd.read_csv('/Users/ahmed/Downloads/computerscience/Kaggle/geoai-ground-level-no2-estimation/geoai-ground/data/Train.csv') \n",
        "test = pd.read_csv('/Users/ahmed/Downloads/computerscience/Kaggle/geoai-ground-level-no2-estimation/geoai-ground/data/Test.csv')\n",
        "groups = train['ID']\n",
        "test_id = test['ID_Zindi']\n",
        "pd.options.display.max_columns = 200\n",
        "#train = train.dropna(axis=0)\n",
        "#test = test.dropna(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fbZpGujEawaS"
      },
      "outputs": [],
      "source": [
        "train = train[train['Precipitation'] < 112.000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:07.984596Z",
          "iopub.status.busy": "2024-07-14T13:27:07.984154Z",
          "iopub.status.idle": "2024-07-14T13:27:08.02151Z",
          "shell.execute_reply": "2024-07-14T13:27:08.020297Z",
          "shell.execute_reply.started": "2024-07-14T13:27:07.984557Z"
        },
        "id": "Rg8w61uzA2k3",
        "outputId": "168eaf2b-8a70-41aa-d066-1a53421376ac",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ID_Zindi                  0\n",
              "Date                      0\n",
              "ID                        0\n",
              "LAT                       0\n",
              "LON                       0\n",
              "Precipitation             0\n",
              "LST                   37593\n",
              "AAI                   12117\n",
              "CloudFraction         12117\n",
              "NO2_strat             12117\n",
              "NO2_total             12117\n",
              "NO2_trop              33428\n",
              "TropopausePressure    12117\n",
              "GT_NO2                    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = train.dropna(subset=['GT_NO2'])\n",
        "train.isnull().sum()\n",
        "#in order to the catboost model to be evaluated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTg6C2AgA2k4"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:08.03265Z",
          "iopub.status.busy": "2024-07-14T13:27:08.032298Z",
          "iopub.status.idle": "2024-07-14T13:27:08.040844Z",
          "shell.execute_reply": "2024-07-14T13:27:08.039708Z",
          "shell.execute_reply.started": "2024-07-14T13:27:08.03262Z"
        },
        "id": "dxYxybSVA2k4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_feats = train.select_dtypes(include=['float'])\n",
        "kmeans = KMeans(n_clusters=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:08.042411Z",
          "iopub.status.busy": "2024-07-14T13:27:08.041968Z",
          "iopub.status.idle": "2024-07-14T13:27:11.331717Z",
          "shell.execute_reply": "2024-07-14T13:27:11.330532Z",
          "shell.execute_reply.started": "2024-07-14T13:27:08.042375Z"
        },
        "id": "Q4vR1sbXA2k4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lat_min, lat_max = 44.92469405, 45.88973369\n",
        "lon_min, lon_max = 8.736496578, 12.59068235\n",
        "\n",
        "num_clusters_lat = 3\n",
        "num_clusters_lon = 4\n",
        "lat_step = (lat_max - lat_min) / num_clusters_lat\n",
        "lon_step = (lon_max - lon_min) / num_clusters_lon\n",
        "def assign_clusters(row, lat_step, lon_step, lat_min, lon_min):\n",
        "    lat_cluster = int((row['LAT'] - lat_min) / lat_step)\n",
        "    lon_cluster = int((row['LON'] - lon_min) / lon_step)\n",
        "    return lat_cluster, lon_cluster\n",
        "for dataset in (train, test):\n",
        "    dataset[['lat_cluster', 'lon_cluster']] = dataset.apply(\n",
        "        assign_clusters, axis=1, result_type='expand',\n",
        "        lat_step=lat_step, lon_step=lon_step, lat_min=lat_min, lon_min=lon_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:11.361234Z",
          "iopub.status.busy": "2024-07-14T13:27:11.360855Z",
          "iopub.status.idle": "2024-07-14T13:27:11.529134Z",
          "shell.execute_reply": "2024-07-14T13:27:11.527859Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.361203Z"
        },
        "id": "w-UjVVwUA2k5",
        "outputId": "8244e416-54d8-4437-b452-fde6588db3ce",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/4c/h16036bx2ync7wqkqvrl2p1h0000gn/T/ipykernel_35343/880121145.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n",
            "/var/folders/4c/h16036bx2ync7wqkqvrl2p1h0000gn/T/ipykernel_35343/880121145.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n"
          ]
        }
      ],
      "source": [
        "\"\"\"train = train.sort_values('Date').reset_index(drop=True)\n",
        "test = test.sort_values('Date').reset_index(drop=True)\n",
        "\"\"\"\n",
        "for df in (train, test):\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "for df in (train, test):\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Month_Day'] = df['Month'].astype(str) + '-' + df['Day'].astype(str)\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Weekday'] = df['Date'].dt.weekday\n",
        "    df['Year_Week'] = df['Year'].astype(str) + '-' + df['Weekday'].astype(str)\n",
        "    df.drop(columns=['Weekday', 'Year', 'Day', 'ID_Zindi'], inplace=True)\n",
        "\n",
        "# Linterations\n",
        "for df in(train,test):\n",
        "    df['Month_lag1'] = df['Month'].shift(1)\n",
        "    df['Month_lag2'] = df['Month'].shift(2)\n",
        "\n",
        "    #df['Month_lag3'] = df['Month'].shift(3)\n",
        "    #df['MonthDay_lag1'] = df['Month_Day'].shift(1)\n",
        "    #df['NO2_strat_lag_1'] = df['NO2_strat'].shift(1)\n",
        "    #df['NO2_strat_lag_7'] = df['NO2_strat'].shift(7)\n",
        "    #df['Precipitation_inter1'] = df['TropopausePressure'] + df['Precipitation']\n",
        "    #df['Precipitation_log'] =  np.log(df['Precipitation'] + 1e-9)\n",
        "    #df['CloudFraction_diff'] = df['CloudFraction'] / df['NO2_strat']\n",
        "    #df['CloudFraction_diff2'] = df['CloudFraction'] / df['NO2_total']\n",
        "    #['Precipitation_fractional'] = df['Precipitation'] * 0.00001\n",
        "    #df['TropopausePressure_fractional'] = round(df['TropopausePressure'] * 0.00001,2)\n",
        "    #df['cluster'] = kmeans.fit_transform(df[['LAT', 'LON']])\n",
        "    #df['cloud_fraction1'] = df['CloudFraction'] % df['NO2_strat']\n",
        "    #df['cloud_fraction2'] = df['CloudFraction'] % df['NO2_total']\n",
        "\n",
        "#for df in(train,test):\n",
        "    #Rolling (Moving Average)\n",
        "    #df['feature_rolling_3_mean'] = df['TropopausePressure'].rolling(5).mean()\n",
        "    #df['feature_rolling_7_mean'] = df['TropopausePressure'].rolling(7).mean()\n",
        "    #df['feature_rolling_7_std'] = df['TropopausePressure'].rolling(7).std()\n",
        "    #df['NO2_strat_rolling7'] = df['NO2_strat'].rolling(7).mean()\n",
        "    #df['NO2_strat_rolling30'] = df['NO2_strat'].rolling(30).mean()\n",
        "    #df['NO2_strat_rolling9'] = df['NO2_strat'].rolling(9).mean()\n",
        "#statsitics of similar variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "y8DWD6OAF-ud",
        "outputId": "76e8ae53-f526-4f46-87a5-893e3d44834d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndef MeanSd(feature1, feature2):\\n    for dataset in (train,test):\\n        dataset[\"SD\" + feature1] = dataset[[feature1,feature2]].std(axis=1)\\n        dataset[\"MEAN\" + feature1] = dataset[[feature1,feature2]].mean(axis=1)\\n\\nMeanSd(\\'NO2_total\\',\\'NO2_strat\\')\\nMeanSd(\\'LON\\',\\'LAT\\')'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "def MeanSd(feature1, feature2):\n",
        "    for dataset in (train,test):\n",
        "        dataset[\"SD\" + feature1] = dataset[[feature1,feature2]].std(axis=1)\n",
        "        dataset[\"MEAN\" + feature1] = dataset[[feature1,feature2]].mean(axis=1)\n",
        "\n",
        "MeanSd('NO2_total','NO2_strat')\n",
        "MeanSd('LON','LAT')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lx9eK7IXquxu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Make sure to fill NA values if any due to rolling and shifting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:11.531198Z",
          "iopub.status.busy": "2024-07-14T13:27:11.530748Z",
          "iopub.status.idle": "2024-07-14T13:27:11.565633Z",
          "shell.execute_reply": "2024-07-14T13:27:11.564625Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.53115Z"
        },
        "id": "oXVjF7-PA2k6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#for df in(train,test):\n",
        "    #df['LST_mean_60'] = df['LST'].rolling(60).mean()\n",
        "    #df['Prec_mean_60'] = df['Precipitation'].rolling(60).mean()\n",
        "    #df['AII_mean'] = df['AAI'].rolling(60).mean()\n",
        "    #df['no2_total'] = df['NO2_total'].rolling(60).min()\n",
        "\n",
        "    #df['feature_rolling_3_mean'] = df['TropopausePressure'].rolling(5).mean()\n",
        "    #df['feature_rolling_7_mean'] = df['TropopausePressure'].rolling(7).mean()\n",
        "    #df['feature_rolling_7_std'] = df['TropopausePressure'].rolling(7).std()\n",
        "    #df['NO2_strat_rolling7'] = df['NO2_strat'].rolling(7).mean()\n",
        "    #df['NO2_strat_rolling30'] = df['NO2_strat'].rolling(30).mean()\n",
        "    #df['NO2_strat_rolling2'] = df['NO2_strat'].rolling(9).mean()\n",
        "#for df in(train,test):\n",
        "  #df['Precipitation_roll_max30'] = df['Precipitation'].rolling(30).max()\n",
        "  #df['Precipitation_roll_mean30'] = df['Precipitation'].rolling(30).mean()\n",
        "  #df['Precipitation_roll_mean60'] = df['Precipitation'].rolling(60).mean()\n",
        "def rolling(feature):\n",
        "    for dataset in (train,test):\n",
        "        #dataset[f\"{feature}_rolling_mean_60\"] = dataset[feature].rolling(60).mean()\n",
        "        dataset[f\"{feature}_rolling_max_60\"] = dataset[feature].rolling(60).max()\n",
        "        #dataset[f\"{feature}_rolling_max_30\"] = dataset[feature].rolling(30).max()\n",
        "        #dataset[f\"{feature}_rolling_min_60\"] = dataset[feature].rolling(60).min()\n",
        "\n",
        "rolling('NO2_trop')\n",
        "rolling('NO2_total')\n",
        "rolling('TropopausePressure')\n",
        "rolling('CloudFraction')\n",
        "#rolling('AAI')\n",
        "#rolling('LST')\n",
        "rolling('Precipitation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8bwdOoRA2k6"
      },
      "source": [
        "# Missing Values & Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:11.579757Z",
          "iopub.status.busy": "2024-07-14T13:27:11.579374Z",
          "iopub.status.idle": "2024-07-14T13:27:11.597165Z",
          "shell.execute_reply": "2024-07-14T13:27:11.596076Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.579716Z"
        },
        "id": "9Q7DTxHNA2k6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "groups = train['ID']\n",
        "for df in(train,test):\n",
        "    df.drop(columns=[\"Date\",'ID','Precipitation','CloudFraction','AAI','lat_cluster'], axis=1,inplace=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "for df in(train,test):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = le.fit_transform(df[col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KELbHAFzA2k7"
      },
      "source": [
        "# CV and Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:11.644891Z",
          "iopub.status.busy": "2024-07-14T13:27:11.644483Z",
          "iopub.status.idle": "2024-07-14T13:27:11.660164Z",
          "shell.execute_reply": "2024-07-14T13:27:11.65905Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.644856Z"
        },
        "id": "cpuC0D5VA2k8",
        "outputId": "ce8cbded-1493-4557-ea33-2c20f3e4e835",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['LAT', 'LON', 'LST', 'NO2_strat', 'NO2_total', 'NO2_trop',\n",
              "       'TropopausePressure', 'GT_NO2', 'lon_cluster', 'Month', 'Month_Day',\n",
              "       'Year_Week', 'Month_lag1', 'Month_lag2', 'NO2_trop_rolling_max_60',\n",
              "       'NO2_total_rolling_max_60', 'TropopausePressure_rolling_max_60',\n",
              "       'CloudFraction_rolling_max_60', 'Precipitation_rolling_max_60'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:11.662298Z",
          "iopub.status.busy": "2024-07-14T13:27:11.661856Z",
          "iopub.status.idle": "2024-07-14T13:27:58.266503Z",
          "shell.execute_reply": "2024-07-14T13:27:58.264922Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.662257Z"
        },
        "id": "HHaRM8N7A2k8",
        "outputId": "c6b8dc0a-075b-405e-e782-13e48f6b52ac",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#model =  LGBMRegressor(random_state=7)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBRFRegressor\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#model = XGBRFRegressor(random_state=7)\u001b[39;00m\n",
            "File \u001b[0;32m~/Downloads/computerscience/Kaggle/geoai-ground-level-no2-estimation/geoai-ground/venv-geoai-ground-level-no2-estimation/lib/python3.12/site-packages/catboost/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     FeaturesData, EFstrType, EShapCalcType, EFeaturesSelectionAlgorithm, EFeaturesSelectionGrouping,\n\u001b[1;32m      3\u001b[0m     Pool, CatBoost, CatBoostClassifier, CatBoostRegressor, CatBoostRanker, CatBoostError, cv, sample_gaussian_process, train,\n\u001b[1;32m      4\u001b[0m     sum_models, _have_equal_features, to_regressor, to_classifier, to_ranker, MultiRegressionCustomMetric,\n\u001b[1;32m      5\u001b[0m     MultiRegressionCustomObjective, MultiTargetCustomMetric, MultiTargetCustomObjective\n\u001b[1;32m      6\u001b[0m )  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VERSION \u001b[38;5;28;01mas\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeaturesData\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEFstrType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEShapCalcType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEFeaturesSelectionAlgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEFeaturesSelectionGrouping\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoostClassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoostRegressor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoostRanker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatboostError\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiTargetCustomMetric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiTargetCustomObjective\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m ]\n",
            "File \u001b[0;32m~/Downloads/computerscience/Kaggle/geoai-ground-level-no2-estimation/geoai-ground/venv-geoai-ground-level-no2-estimation/lib/python3.12/site-packages/catboost/core.py:45\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_plot_file, try_plot_offline, OfflineMetricVisualizer\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _catboost\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BuiltinMetric\n",
            "File \u001b[0;32m~/Downloads/computerscience/Kaggle/geoai-ground-level-no2-estimation/geoai-ground/venv-geoai-ground-level-no2-estimation/lib/python3.12/site-packages/catboost/plot_helpers.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _catboost\n\u001b[1;32m      6\u001b[0m fspath \u001b[38;5;241m=\u001b[39m _catboost\u001b[38;5;241m.\u001b[39mfspath\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_plot_offline\u001b[39m(figs):\n",
            "File \u001b[0;32m_catboost.pyx:1\u001b[0m, in \u001b[0;36minit _catboost\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "#model =  LGBMRegressor(random_state=7)\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRFRegressor\n",
        "model = XGBRFRegressor(random_state=7)\n",
        "\"\"\"model = CatBoostRegressor(\n",
        "    iterations=1000,        # Number of boosting iterations\n",
        "    learning_rate=0.1,      # Learning rate\n",
        "    depth=6,                # Depth of the tree\n",
        "    loss_function='RMSE',   # Loss function\n",
        "    eval_metric='RMSE',     # Evaluation metric\n",
        "    random_seed=7,          # Random seed for reproducibility\n",
        "    verbose=100             # Print information every 100 iterations\n",
        ")\"\"\"\n",
        "\n",
        "#model = CatBoostRegressor(random_state=7)\n",
        "#model = XGBRegressor(random_state= 7)\n",
        "n_splits = 5\n",
        "n = train['GT_NO2'].count()\n",
        "num_bins = int(1 + np.log2(n))\n",
        "cv = GroupKFold(n_splits=n_splits)\n",
        "\n",
        "def validate(trainset, testset, target_col):\n",
        "\n",
        "    model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    print('std:', valid_testset.std())\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    print('score:', score)\n",
        "\n",
        "    return score\n",
        "stds = []\n",
        "rmse = []\n",
        "\n",
        "for train_idx, test_idx in cv.split(train.drop(columns='GT_NO2'), train['GT_NO2'], groups=groups):\n",
        "    train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "    stds.append(test_v['GT_NO2'].std())\n",
        "    rmse.append(validate(train_v, test_v, 'GT_NO2'))\n",
        "\n",
        "print('RMSE:', np.array(rmse).mean())\n",
        "print('RMSE std:', np.array(rmse).std())\n",
        "print('Standard Deviations:', stds)\n",
        "print('RMSEs Deviations:', rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plzHI6cnKA3O"
      },
      "outputs": [],
      "source": [
        "#9.83178851079467 the month day feature added\n",
        "#9.666401979691653 best of lb = 8.592019681\n",
        "#9.70687071038439 on testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iJ5kLVxnAgF"
      },
      "outputs": [],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.268549Z",
          "iopub.status.busy": "2024-07-14T13:27:58.268096Z",
          "iopub.status.idle": "2024-07-14T13:27:58.273759Z",
          "shell.execute_reply": "2024-07-14T13:27:58.272513Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.268508Z"
        },
        "id": "v_2Fi9rJA2k8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#10.155311364148783 catboost leaderboard best s far 9.525616244\n",
        "#9.920018431939276 catboost and added the day features - probably it overfits - indeed\n",
        "#9.43514150859055 removing all missing values leaderboard 11.9315086\n",
        "#10.139203906659699\n",
        "#10.201862135497844 the effect of mutual information\n",
        "#9.476601295728365\n",
        "#10.025125392453269"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.275771Z",
          "iopub.status.busy": "2024-07-14T13:27:58.275324Z",
          "iopub.status.idle": "2024-07-14T13:27:58.284828Z",
          "shell.execute_reply": "2024-07-14T13:27:58.283586Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.275732Z"
        },
        "id": "FK5Eig4FA2k8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#11.581549939504997 with month ,\n",
        "#12.246931687286574\n",
        "#12.235259341356834\n",
        "#12.231869616518921\n",
        "#12.126333476852636\n",
        "#11.728753513719479\n",
        "#11.724263538742687\n",
        "#11.728753513719479\n",
        "#11.711277895239192\n",
        "#11.694341231865433\n",
        "#11.671982426722977\n",
        "#11.642227859275334 without the mean - with the custamized means of rolling\n",
        "#11.314985626750522\n",
        "#11.314985626750522\n",
        "#11.312413659993586\n",
        "#11.135932399851725\n",
        "#11.728753513719479 best so far features = ['LAT', 'Month','NO2_trop_rolling_max_60','NO2_total_rolling_max_60','TropopausePressure_rolling_max_60','CloudFraction_rolling_max_60','Precipitation_rolling_max_60']\n",
        "#11.40320255907346 with the ID LB = 9.571700669\n",
        "#11.597286160951526 the distance feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.323834Z",
          "iopub.status.busy": "2024-07-14T13:27:58.323495Z",
          "iopub.status.idle": "2024-07-14T13:27:58.336636Z",
          "shell.execute_reply": "2024-07-14T13:27:58.335419Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.323805Z"
        },
        "id": "3RdbdtLXA2k9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import contextlib, os,sys\n",
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Using features: Based_features, {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "def feature_combination_analysis(train, target_col, groups, n_splits):\n",
        "    base_features =  ['LAT', 'LON', 'LST', 'NO2_strat', 'NO2_total', 'NO2_trop', 'TropopausePressure', 'lon_cluster', 'Month', 'Month_Day', 'Year_Week', 'Month_lag1', 'Month_lag2', 'NO2_trop_rolling_max_60', 'NO2_total_rolling_max_60', 'TropopausePressure_rolling_max_60', 'CloudFraction_rolling_max_60', 'Precipitation_rolling_max_60']\n",
        "    additional_features = [col for col in train.drop(columns=target_col).columns if col not in base_features]\n",
        "\n",
        "    feature_importances = {}\n",
        "    for col in additional_features:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train[[target_col] + base_features + [col]], train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v[[target_col] + base_features + [col]], test_v[[target_col] + base_features + [col]], target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} with base features, RMSE: {feature_rmse}')\n",
        "    sorted_features = sorted(feature_importances.items(), key=lambda x: x[1])\n",
        "\n",
        "    print('Feature importances:')\n",
        "    for feature, importance in sorted_features:\n",
        "        print(f'{feature}: {importance}')\n",
        "\n",
        "    return feature_importances\n",
        "feature_importances = feature_combination_analysis(train, 'GT_NO2', groups, n_splits)\n",
        "#9.6664019796\n",
        "##9.666401979691653"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.286545Z",
          "iopub.status.busy": "2024-07-14T13:27:58.286156Z",
          "iopub.status.idle": "2024-07-14T13:27:58.304718Z",
          "shell.execute_reply": "2024-07-14T13:27:58.303412Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.286509Z"
        },
        "id": "eaUv4zbZA2k8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import contextlib, os,sys\n",
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Removed feature: {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "\n",
        "def lofo_analysis(train, target_col, groups, n_splits):\n",
        "    base_rmse = []\n",
        "    for train_idx, test_idx in cv.split(train.drop(columns=target_col), train[target_col], groups=groups):\n",
        "        train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "        base_rmse.append(validate(train_v, test_v, target_col))\n",
        "    base_score = np.array(base_rmse).mean()\n",
        "    print('Base RMSE:', base_score)\n",
        "\n",
        "    feature_importances = {}\n",
        "    for col in train.drop(columns=target_col).columns:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train.drop(columns=[target_col, col]), train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v.drop(columns=col), test_v.drop(columns=col), target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} removed, RMSE: {feature_rmse}')\n",
        "\n",
        "    bad_features = [col for col in feature_importances if feature_importances[col] > base_score]\n",
        "    good_features = [col for col in feature_importances if feature_importances[col] <= base_score]\n",
        "\n",
        "    print('Good features:', good_features)\n",
        "    print('Bad features:', bad_features)\n",
        "\n",
        "    return good_features, bad_features, feature_importances\n",
        "\n",
        "good_features, bad_features, feature_importances = lofo_analysis(train, 'GT_NO2', groups, n_splits)\n",
        "print('Feature importances:')\n",
        "for feature, importance in feature_importances.items():\n",
        "    print(f'{feature}: {importance}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.307314Z",
          "iopub.status.busy": "2024-07-14T13:27:58.306385Z",
          "iopub.status.idle": "2024-07-14T13:27:58.322219Z",
          "shell.execute_reply": "2024-07-14T13:27:58.321143Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.307268Z"
        },
        "id": "oKULOH3_A2k9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bad_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.339441Z",
          "iopub.status.busy": "2024-07-14T13:27:58.338904Z",
          "iopub.status.idle": "2024-07-14T13:27:58.353303Z",
          "shell.execute_reply": "2024-07-14T13:27:58.352146Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.3394Z"
        },
        "id": "iaxeV2MtA2k9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Using feature: {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "\n",
        "def single_feature_analysis(train, target_col, groups, n_splits):\n",
        "    feature_importances = {}\n",
        "    for col in train.drop(columns=target_col).columns:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train[[target_col, col]], train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v[[target_col, col]], test_v[[target_col, col]], target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} only, RMSE: {feature_rmse}')\n",
        "    sorted_features = sorted(feature_importances.items(), key=lambda x: x[1])\n",
        "\n",
        "    print('Feature importances:')\n",
        "    for feature, importance in sorted_features:\n",
        "        print(f'{feature}: {importance}')\n",
        "\n",
        "    return feature_importances\n",
        "feature_importances = single_feature_analysis(train, 'GT_NO2', groups, n_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.355125Z",
          "iopub.status.busy": "2024-07-14T13:27:58.354742Z",
          "iopub.status.idle": "2024-07-14T13:27:58.388466Z",
          "shell.execute_reply": "2024-07-14T13:27:58.38673Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.355094Z"
        },
        "id": "cu-5huLAA2k9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "hshsd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:32:00.517132Z",
          "iopub.status.busy": "2024-07-14T13:32:00.516715Z",
          "iopub.status.idle": "2024-07-14T13:32:11.521785Z",
          "shell.execute_reply": "2024-07-14T13:32:11.52073Z",
          "shell.execute_reply.started": "2024-07-14T13:32:00.517099Z"
        },
        "id": "cp0d4T3DA2k-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.fit(train.drop(columns='GT_NO2'),train['GT_NO2'])\n",
        "y_pred = model.predict(test)\n",
        "sub_df = pd.DataFrame({'id': test_id,'GT_NO2':y_pred})\n",
        "sub_df.to_csv('submission9617522430249442.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:33:07.383755Z",
          "iopub.status.busy": "2024-07-14T13:33:07.383329Z",
          "iopub.status.idle": "2024-07-14T13:33:07.689903Z",
          "shell.execute_reply": "2024-07-14T13:33:07.688533Z",
          "shell.execute_reply.started": "2024-07-14T13:33:07.383723Z"
        },
        "id": "9m6QE2TgA2k-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "importances = model.feature_importances_\n",
        "names = model.feature_names_\n",
        "fi = pd.DataFrame({'Feature': names,\n",
        "                   'importances': importances})\n",
        "fi = fi.sort_values(by='importances', ascending=False)\n",
        "\n",
        "fi.plot(kind='bar', x='Feature', y='importances', legend=False, figsize=(10, 6))\n",
        "plt.title('Feature Importances')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:33:32.869208Z",
          "iopub.status.busy": "2024-07-14T13:33:32.86811Z",
          "iopub.status.idle": "2024-07-14T13:33:32.876484Z",
          "shell.execute_reply": "2024-07-14T13:33:32.875389Z",
          "shell.execute_reply.started": "2024-07-14T13:33:32.869166Z"
        },
        "id": "NLSlX2mtA2k-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yLOUsVtXwyQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 5270243,
          "sourceId": 8770223,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30746,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
