{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "863HHJcDA2kz",
        "outputId": "5b96ce4e-3fc8-4cd5-836f-b0e23cb5fafb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading zindi-geoai-ground-level-no2-estimation-challenge, 3366877 bytes compressed\n",
            "[==================================================] 3366877 bytes downloaded\n",
            "Downloaded and uncompressed: zindi-geoai-ground-level-no2-estimation-challenge\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'zindi-geoai-ground-level-no2-estimation-challenge:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5270243%2F8770223%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240714%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240714T134437Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D31f3bd823e21cedac111978a1e8d9524d468e12ea624e97e1568e04c793706e994e0ff9940d6e19e24a0e365e270f135f1eca47e67e4d5d13695d1cb703bfbc688fe2397bd8ffd1024ec64f7e5edc59b8dd6f25ad68240375c2ccafc1ae35a8da0374dcc71c0eef26a115040520d215071fa7c5bb8442099b5691ddee9f77e07985f70e7c6af2ff4ff5238f38582b5549ed42fdae8662597d519b8f33a405bd753ec946d20eec12316341410bf35aaa71352ed9d18fd94405663f610647685d0ff1b8ff26a320b940cb9bc1ed0994c40ac1fe5f9f9ff00df0131686d88cf3cc644e65d3eabef7ff8aaef7f9e658cf8dd37cc12e73a4cdba6ea01f73108ad2a61'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbxChli6A2k1"
      },
      "source": [
        "# Importings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86s-4DVxA2k2"
      },
      "source": [
        "- mutual information\n",
        "- search for the catboost regressor on kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:07.74247Z",
          "iopub.status.busy": "2024-07-14T13:27:07.741877Z",
          "iopub.status.idle": "2024-07-14T13:27:07.982266Z",
          "shell.execute_reply": "2024-07-14T13:27:07.980998Z",
          "shell.execute_reply.started": "2024-07-14T13:27:07.742426Z"
        },
        "id": "wfN_LZD2A2k2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd                                    # for data\n",
        "import numpy as np                                     # for math\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error         # Regressortion metric\n",
        "from sklearn.model_selection import GroupKFold,KFold, TimeSeriesSplit   # for validation\n",
        "from sklearn.preprocessing import LabelEncoder         # for encoding\n",
        "import sklearn.manifold._t_sne as tsne                 # for t_sne\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import TimeSeriesSplit# for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from matplotlib import rcParams\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from scipy.stats import rankdata\n",
        "import xgboost as xgb\n",
        "from sklearn.cluster import KMeans\n",
        "from path import Path\n",
        "path = Path('/kaggle/input/zindi-geoai-ground-level-no2-estimation-challenge')\n",
        "train = pd.read_csv(path /'Train.csv')\n",
        "test = pd.read_csv(path /'Test.csv')\n",
        "groups = train['ID']\n",
        "test_id = test['ID_Zindi']\n",
        "pd.options.display.max_columns = 200\n",
        "#train = train.dropna(axis=0)\n",
        "#test = test.dropna(axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:07.984596Z",
          "iopub.status.busy": "2024-07-14T13:27:07.984154Z",
          "iopub.status.idle": "2024-07-14T13:27:08.02151Z",
          "shell.execute_reply": "2024-07-14T13:27:08.020297Z",
          "shell.execute_reply.started": "2024-07-14T13:27:07.984557Z"
        },
        "id": "Rg8w61uzA2k3",
        "outputId": "6cca80e4-697d-4b23-894e-54b5667e55da",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ID_Zindi                  0\n",
              "Date                      0\n",
              "ID                        0\n",
              "LAT                       0\n",
              "LON                       0\n",
              "Precipitation             0\n",
              "LST                   37594\n",
              "AAI                   12118\n",
              "CloudFraction         12118\n",
              "NO2_strat             12118\n",
              "NO2_total             12118\n",
              "NO2_trop              33429\n",
              "TropopausePressure    12118\n",
              "GT_NO2                    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = train.dropna(subset=['GT_NO2'])\n",
        "train.isnull().sum()\n",
        "#in order to the catboost model to be evaluated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTg6C2AgA2k4"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:08.022969Z",
          "iopub.status.busy": "2024-07-14T13:27:08.022665Z",
          "iopub.status.idle": "2024-07-14T13:27:08.02988Z",
          "shell.execute_reply": "2024-07-14T13:27:08.028643Z",
          "shell.execute_reply.started": "2024-07-14T13:27:08.022945Z"
        },
        "id": "kKBAekNnA2k4",
        "outputId": "948f6fa1-3dc3-4141-e8dc-0dede6639b35",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"agg_funcs = ['mean', 'std', 'min', 'max']\\ndef add_aggregated_features(dataset, columns, funcs):\\n    dataset = dataset.copy()\\n    for column in columns:\\n        agg_features_id = dataset.groupby('ID')[column].agg(funcs)\\n        agg_features_id.columns = [f'{column}_{agg_func}_agg_ID' for agg_func in funcs]\\n        dataset = dataset.merge(agg_features_id, on='ID')\\n    return dataset\\ntrain = add_aggregated_features(train, columns_to_aggregate, agg_funcs)\\ntest = add_aggregated_features(test, columns_to_aggregate, agg_funcs)\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"agg_funcs = ['mean', 'std', 'min', 'max']\n",
        "def add_aggregated_features(dataset, columns, funcs):\n",
        "    dataset = dataset.copy()\n",
        "    for column in columns:\n",
        "        agg_features_id = dataset.groupby('ID')[column].agg(funcs)\n",
        "        agg_features_id.columns = [f'{column}_{agg_func}_agg_ID' for agg_func in funcs]\n",
        "        dataset = dataset.merge(agg_features_id, on='ID')\n",
        "    return dataset\n",
        "train = add_aggregated_features(train, columns_to_aggregate, agg_funcs)\n",
        "test = add_aggregated_features(test, columns_to_aggregate, agg_funcs)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:08.03265Z",
          "iopub.status.busy": "2024-07-14T13:27:08.032298Z",
          "iopub.status.idle": "2024-07-14T13:27:08.040844Z",
          "shell.execute_reply": "2024-07-14T13:27:08.039708Z",
          "shell.execute_reply.started": "2024-07-14T13:27:08.03262Z"
        },
        "id": "dxYxybSVA2k4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_feats = train.select_dtypes(include=['float'])\n",
        "kmeans = KMeans(n_clusters=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:08.042411Z",
          "iopub.status.busy": "2024-07-14T13:27:08.041968Z",
          "iopub.status.idle": "2024-07-14T13:27:11.331717Z",
          "shell.execute_reply": "2024-07-14T13:27:11.330532Z",
          "shell.execute_reply.started": "2024-07-14T13:27:08.042375Z"
        },
        "id": "Q4vR1sbXA2k4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lat_min, lat_max = 44.92469405, 45.88973369\n",
        "lon_min, lon_max = 8.736496578, 12.59068235\n",
        "\n",
        "num_clusters_lat = 3\n",
        "num_clusters_lon = 4\n",
        "lat_step = (lat_max - lat_min) / num_clusters_lat\n",
        "lon_step = (lon_max - lon_min) / num_clusters_lon\n",
        "def assign_clusters(row, lat_step, lon_step, lat_min, lon_min):\n",
        "    lat_cluster = int((row['LAT'] - lat_min) / lat_step)\n",
        "    lon_cluster = int((row['LON'] - lon_min) / lon_step)\n",
        "    return lat_cluster, lon_cluster\n",
        "for dataset in (train, test):\n",
        "    dataset[['lat_cluster', 'lon_cluster']] = dataset.apply(\n",
        "        assign_clusters, axis=1, result_type='expand',\n",
        "        lat_step=lat_step, lon_step=lon_step, lat_min=lat_min, lon_min=lon_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:11.361234Z",
          "iopub.status.busy": "2024-07-14T13:27:11.360855Z",
          "iopub.status.idle": "2024-07-14T13:27:11.529134Z",
          "shell.execute_reply": "2024-07-14T13:27:11.527859Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.361203Z"
        },
        "id": "w-UjVVwUA2k5",
        "outputId": "a7ea465f-cc7b-45e1-99a5-d789dbd6d060",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-32-a60839a130cf>:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'def MeanSd(feature1, feature2):\\n    for dataset in (train,test):\\n        dataset[\"SD\" + feature1] = dataset[[feature1,feature2]].std(axis=1)\\n        dataset[\"MEAN\" + feature1] = dataset[[feature1,feature2]].mean(axis=1)\\n\\nMeanSd(\\'NO2_trop\\',\\'NO2_total\\')\\nMeanSd(\\'LON\\',\\'LAT\\')'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"train = train.sort_values('Date').reset_index(drop=True)\n",
        "test = test.sort_values('Date').reset_index(drop=True)\n",
        "\"\"\"\n",
        "for df in (train,test):\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df['Day'] =  df['Date'].dt.day\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Month_Day'] = df['Month'].astype(str) + '-' + df['Day'].astype(str)\n",
        "    #df['target_month'] = df['Month'].map(df[['Month','GT_NO2']].groupby('Month')['GT_NO2'].mean())\n",
        "    #df['Month_start'] = df['Date'].dt.is_month_start\n",
        "    #df['Year'] =  df['Date'].dt.year\n",
        "    #df['Week'] =  df['Date'].dt.weekday\n",
        "    #df['Year_Week'] = df['Year'].astype(str) + '-' + df['Week'].astype(str)\n",
        "    #df.set_index(df['Date'],inplace=True)\n",
        "    #df['DayOfWeek'] =  df['Date'].dt.dayofweek\n",
        "    df.drop(columns=['Week','Year','Day','ID_Zindi'],inplace=True)\n",
        "\n",
        "#Linterations\n",
        "for df in(train,test):\n",
        "    df['Month_lag1'] = df['Month'].shift(1)\n",
        "    #df['Month_lag2'] = df['Month'].shift(2)\n",
        "    #df['NO2_strat_lag_1'] = df['NO2_strat'].shift(1)\n",
        "    #df['NO2_strat_lag_7'] = df['NO2_strat'].shift(7)\n",
        "    #df['Precipitation_inter1'] = df['TropopausePressure'] + df['Precipitation']\n",
        "    #df['Precipitation_log'] =  np.log(df['Precipitation'] + 1e-9)\n",
        "    #df['CloudFraction_diff'] = df['CloudFraction'] / df['NO2_strat']\n",
        "    #df['CloudFraction_diff2'] = df['CloudFraction'] / df['NO2_total']\n",
        "    #['Precipitation_fractional'] = df['Precipitation'] * 0.00001\n",
        "    #df['TropopausePressure_fractional'] = round(df['TropopausePressure'] * 0.00001,2)\n",
        "    #df['cluster'] = kmeans.fit_transform(df[['LAT', 'LON']])\n",
        "    #df['cloud_fraction1'] = df['CloudFraction'] % df['NO2_strat']\n",
        "    #df['cloud_fraction2'] = df['CloudFraction'] % df['NO2_total']\n",
        "\n",
        "\n",
        "#for df in(train,test):\n",
        "    #Rolling (Moving Average)\n",
        "    #df['feature_rolling_3_mean'] = df['TropopausePressure'].rolling(5).mean()\n",
        "    #df['feature_rolling_7_mean'] = df['TropopausePressure'].rolling(7).mean()\n",
        "    #df['feature_rolling_7_std'] = df['TropopausePressure'].rolling(7).std()\n",
        "    #df['NO2_strat_rolling7'] = df['NO2_strat'].rolling(7).mean()\n",
        "    #df['NO2_strat_rolling30'] = df['NO2_strat'].rolling(30).mean()\n",
        "    #df['NO2_strat_rolling2'] = df['NO2_strat'].rolling(9).mean()\n",
        "#statsitics of similar variables\n",
        "\n",
        "\"\"\"def MeanSd(feature1, feature2):\n",
        "    for dataset in (train,test):\n",
        "        dataset[\"SD\" + feature1] = dataset[[feature1,feature2]].std(axis=1)\n",
        "        dataset[\"MEAN\" + feature1] = dataset[[feature1,feature2]].mean(axis=1)\n",
        "\n",
        "MeanSd('NO2_trop','NO2_total')\n",
        "MeanSd('LON','LAT')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:11.531198Z",
          "iopub.status.busy": "2024-07-14T13:27:11.530748Z",
          "iopub.status.idle": "2024-07-14T13:27:11.565633Z",
          "shell.execute_reply": "2024-07-14T13:27:11.564625Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.53115Z"
        },
        "id": "oXVjF7-PA2k6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#for df in(train,test):\n",
        "    #df['LST_mean_60'] = df['LST'].rolling(60).mean()\n",
        "    #df['Prec_mean_60'] = df['Precipitation'].rolling(60).mean()\n",
        "    #df['AII_mean'] = df['AAI'].rolling(60).mean()\n",
        "    #df['no2_total'] = df['NO2_total'].rolling(60).min()\n",
        "\n",
        "    #df['feature_rolling_3_mean'] = df['TropopausePressure'].rolling(5).mean()\n",
        "    #df['feature_rolling_7_mean'] = df['TropopausePressure'].rolling(7).mean()\n",
        "    #df['feature_rolling_7_std'] = df['TropopausePressure'].rolling(7).std()\n",
        "    #df['NO2_strat_rolling7'] = df['NO2_strat'].rolling(7).mean()\n",
        "    #df['NO2_strat_rolling30'] = df['NO2_strat'].rolling(30).mean()\n",
        "    #df['NO2_strat_rolling2'] = df['NO2_strat'].rolling(9).mean()\n",
        "\n",
        "\n",
        "def rolling(feature):\n",
        "    for dataset in (train,test):\n",
        "        #dataset[f\"{feature}_rolling_mean_60\"] = dataset[feature].rolling(60).mean()\n",
        "        dataset[f\"{feature}_rolling_max_60\"] = dataset[feature].rolling(60).max()\n",
        "        #dataset[f\"{feature}_rolling_min_60\"] = dataset[feature].rolling(60).min()\n",
        "\n",
        "\n",
        "rolling('NO2_trop')\n",
        "rolling('NO2_total')\n",
        "rolling('TropopausePressure')\n",
        "rolling('CloudFraction')\n",
        "#rolling('AAI')\n",
        "#rolling('LST')\n",
        "rolling('Precipitation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8bwdOoRA2k6"
      },
      "source": [
        "# Missing Values & Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:11.579757Z",
          "iopub.status.busy": "2024-07-14T13:27:11.579374Z",
          "iopub.status.idle": "2024-07-14T13:27:11.597165Z",
          "shell.execute_reply": "2024-07-14T13:27:11.596076Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.579716Z"
        },
        "id": "9Q7DTxHNA2k6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "groups = train['ID']\n",
        "for df in(train,test):\n",
        "    df.drop(columns=[\"Date\",'ID','Precipitation','CloudFraction','AAI','lat_cluster'], axis=1,inplace=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "for df in(train,test):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = le.fit_transform(df[col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KELbHAFzA2k7"
      },
      "source": [
        "# CV and Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:11.644891Z",
          "iopub.status.busy": "2024-07-14T13:27:11.644483Z",
          "iopub.status.idle": "2024-07-14T13:27:11.660164Z",
          "shell.execute_reply": "2024-07-14T13:27:11.65905Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.644856Z"
        },
        "id": "cpuC0D5VA2k8",
        "outputId": "05c526a0-4a36-4795-dcb9-065be5810e08",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['LAT', 'LON', 'LST', 'NO2_strat', 'NO2_total', 'NO2_trop',\n",
              "       'TropopausePressure', 'GT_NO2', 'lon_cluster', 'Month', 'Month_Day',\n",
              "       'Year_Week', 'Month_lag1', 'Month_lag2', 'NO2_trop_rolling_max_60',\n",
              "       'NO2_total_rolling_max_60', 'TropopausePressure_rolling_max_60',\n",
              "       'CloudFraction_rolling_max_60', 'Precipitation_rolling_max_60'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkLkYE7PNB6a"
      },
      "outputs": [],
      "source": [
        "stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:11.662298Z",
          "iopub.status.busy": "2024-07-14T13:27:11.661856Z",
          "iopub.status.idle": "2024-07-14T13:27:58.266503Z",
          "shell.execute_reply": "2024-07-14T13:27:58.264922Z",
          "shell.execute_reply.started": "2024-07-14T13:27:11.662257Z"
        },
        "id": "HHaRM8N7A2k8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#model =  LGBMRegressor(random_state=7)\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRFRegressor\n",
        "#model = XGBRFRegressor(random_state=7)\n",
        "model = CatBoostRegressor(random_state=7)\n",
        "\n",
        "#model = XGBRegressor(random_state= 7)\n",
        "n_splits = 5\n",
        "n = train['GT_NO2'].count()\n",
        "num_bins = int(1 + np.log2(n))\n",
        "cv = GroupKFold(n_splits=n_splits)\n",
        "\n",
        "def validate(trainset, testset, target_col):\n",
        "\n",
        "    model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    print('std:', valid_testset.std())\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    print('score:', score)\n",
        "\n",
        "    return score\n",
        "stds = []\n",
        "rmse = []\n",
        "\n",
        "for train_idx, test_idx in cv.split(train.drop(columns='GT_NO2'), train['GT_NO2'], groups=groups):\n",
        "    train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "    stds.append(test_v['GT_NO2'].std())\n",
        "    rmse.append(validate(train_v, test_v, 'GT_NO2'))\n",
        "\n",
        "print('RMSE:', np.array(rmse).mean())\n",
        "print('RMSE std:', np.array(rmse).std())\n",
        "print('Standard Deviations:', stds)\n",
        "print('RMSEs Deviations:', rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plzHI6cnKA3O"
      },
      "outputs": [],
      "source": [
        "#9.83178851079467 the month day feature added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.268549Z",
          "iopub.status.busy": "2024-07-14T13:27:58.268096Z",
          "iopub.status.idle": "2024-07-14T13:27:58.273759Z",
          "shell.execute_reply": "2024-07-14T13:27:58.272513Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.268508Z"
        },
        "id": "v_2Fi9rJA2k8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#10.155311364148783 catboost leaderboard best s far 9.525616244\n",
        "#9.920018431939276 catboost and added the day features - probably it overfits - indeed\n",
        "#9.43514150859055 removing all missing values leaderboard 11.9315086\n",
        "#10.139203906659699\n",
        "#10.201862135497844 the effect of mutual information\n",
        "#9.476601295728365\n",
        "#10.025125392453269"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.275771Z",
          "iopub.status.busy": "2024-07-14T13:27:58.275324Z",
          "iopub.status.idle": "2024-07-14T13:27:58.284828Z",
          "shell.execute_reply": "2024-07-14T13:27:58.283586Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.275732Z"
        },
        "id": "FK5Eig4FA2k8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#11.581549939504997 with month ,\n",
        "#12.246931687286574\n",
        "#12.235259341356834\n",
        "#12.231869616518921\n",
        "#12.126333476852636\n",
        "#11.728753513719479\n",
        "#11.724263538742687\n",
        "#11.728753513719479\n",
        "#11.711277895239192\n",
        "#11.694341231865433\n",
        "#11.671982426722977\n",
        "#11.642227859275334 without the mean - with the custamized means of rolling\n",
        "#11.314985626750522\n",
        "#11.314985626750522\n",
        "#11.312413659993586\n",
        "#11.135932399851725\n",
        "#11.728753513719479 best so far features = ['LAT', 'Month','NO2_trop_rolling_max_60','NO2_total_rolling_max_60','TropopausePressure_rolling_max_60','CloudFraction_rolling_max_60','Precipitation_rolling_max_60']\n",
        "#11.40320255907346 with the ID LB = 9.571700669\n",
        "#11.597286160951526 the distance feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.323834Z",
          "iopub.status.busy": "2024-07-14T13:27:58.323495Z",
          "iopub.status.idle": "2024-07-14T13:27:58.336636Z",
          "shell.execute_reply": "2024-07-14T13:27:58.335419Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.323805Z"
        },
        "id": "3RdbdtLXA2k9",
        "outputId": "a23b2423-631a-41da-d238-60b79346f72c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating feature: Month_Day\n",
            "Using features: Based_features, Month_Day | Validation MSE: 9.96520145953869\n"
          ]
        }
      ],
      "source": [
        "import contextlib, os,sys\n",
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Using features: Based_features, {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "def feature_combination_analysis(train, target_col, groups, n_splits):\n",
        "    base_features = ['LAT', 'LON', 'LST', 'NO2_strat', 'NO2_total', 'NO2_trop', 'TropopausePressure', 'lon_cluster', 'Month', 'NO2_trop_rolling_max_60', 'NO2_total_rolling_max_60', 'TropopausePressure_rolling_max_60', 'CloudFraction_rolling_max_60', 'Precipitation_rolling_max_60']\n",
        "    additional_features = [col for col in train.drop(columns=target_col).columns if col not in base_features]\n",
        "\n",
        "    feature_importances = {}\n",
        "    for col in additional_features:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train[[target_col] + base_features + [col]], train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v[[target_col] + base_features + [col]], test_v[[target_col] + base_features + [col]], target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} with base features, RMSE: {feature_rmse}')\n",
        "    sorted_features = sorted(feature_importances.items(), key=lambda x: x[1])\n",
        "\n",
        "    print('Feature importances:')\n",
        "    for feature, importance in sorted_features:\n",
        "        print(f'{feature}: {importance}')\n",
        "\n",
        "    return feature_importances\n",
        "feature_importances = feature_combination_analysis(train, 'GT_NO2', groups, n_splits)\n",
        "#10.016482406802602\n",
        "#9.998350582198418 Precipitation_inter1\n",
        "#10.009211550504173 Precipitation_log (risky) train it with the first one to make sure\n",
        "#10.003845293791532 CloudFraction_diff risky\n",
        "#9.997560490888084 tropfactional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.286545Z",
          "iopub.status.busy": "2024-07-14T13:27:58.286156Z",
          "iopub.status.idle": "2024-07-14T13:27:58.304718Z",
          "shell.execute_reply": "2024-07-14T13:27:58.303412Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.286509Z"
        },
        "id": "eaUv4zbZA2k8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import contextlib, os,sys\n",
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Removed feature: {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "\n",
        "def lofo_analysis(train, target_col, groups, n_splits):\n",
        "    base_rmse = []\n",
        "    for train_idx, test_idx in cv.split(train.drop(columns=target_col), train[target_col], groups=groups):\n",
        "        train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "        base_rmse.append(validate(train_v, test_v, target_col))\n",
        "    base_score = np.array(base_rmse).mean()\n",
        "    print('Base RMSE:', base_score)\n",
        "\n",
        "    feature_importances = {}\n",
        "    for col in train.drop(columns=target_col).columns:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train.drop(columns=[target_col, col]), train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v.drop(columns=col), test_v.drop(columns=col), target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} removed, RMSE: {feature_rmse}')\n",
        "\n",
        "    bad_features = [col for col in feature_importances if feature_importances[col] > base_score]\n",
        "    good_features = [col for col in feature_importances if feature_importances[col] <= base_score]\n",
        "\n",
        "    print('Good features:', good_features)\n",
        "    print('Bad features:', bad_features)\n",
        "\n",
        "    return good_features, bad_features, feature_importances\n",
        "\n",
        "good_features, bad_features, feature_importances = lofo_analysis(train, 'GT_NO2', groups, n_splits)\n",
        "\n",
        "print('Feature importances:')\n",
        "for feature, importance in feature_importances.items():\n",
        "    print(f'{feature}: {importance}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.307314Z",
          "iopub.status.busy": "2024-07-14T13:27:58.306385Z",
          "iopub.status.idle": "2024-07-14T13:27:58.322219Z",
          "shell.execute_reply": "2024-07-14T13:27:58.321143Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.307268Z"
        },
        "id": "oKULOH3_A2k9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bad_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.339441Z",
          "iopub.status.busy": "2024-07-14T13:27:58.338904Z",
          "iopub.status.idle": "2024-07-14T13:27:58.353303Z",
          "shell.execute_reply": "2024-07-14T13:27:58.352146Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.3394Z"
        },
        "id": "iaxeV2MtA2k9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\"\"\"@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stdout = devnull\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "def validate(trainset, testset, target_col, feature_name=None):\n",
        "    with suppress_output():\n",
        "        model.fit(trainset.drop(columns=target_col), trainset[target_col])\n",
        "    pred = model.predict(testset.drop(columns=target_col))\n",
        "    valid_idx = testset[target_col].notna()\n",
        "    valid_testset = testset[target_col][valid_idx]\n",
        "    valid_pred = pred[valid_idx]\n",
        "    score = mean_squared_error(valid_testset, valid_pred, squared=False)\n",
        "    if feature_name:\n",
        "        print(f'Using feature: {feature_name} | Validation MSE: {score}')\n",
        "    else:\n",
        "        print(f'Validation MSE: {score}')\n",
        "    return score\n",
        "\n",
        "def single_feature_analysis(train, target_col, groups, n_splits):\n",
        "    feature_importances = {}\n",
        "    for col in train.drop(columns=target_col).columns:\n",
        "        scores = []\n",
        "        print(f'Evaluating feature: {col}')\n",
        "        for train_idx, test_idx in cv.split(train[[target_col, col]], train[target_col], groups=groups):\n",
        "            train_v, test_v = train.iloc[train_idx], train.iloc[test_idx]\n",
        "            scores.append(validate(train_v[[target_col, col]], test_v[[target_col, col]], target_col, col))\n",
        "        feature_rmse = np.array(scores).mean()\n",
        "        feature_importances[col] = feature_rmse\n",
        "        print(f'Feature {col} only, RMSE: {feature_rmse}')\n",
        "    sorted_features = sorted(feature_importances.items(), key=lambda x: x[1])\n",
        "\n",
        "    print('Feature importances:')\n",
        "    for feature, importance in sorted_features:\n",
        "        print(f'{feature}: {importance}')\n",
        "\n",
        "    return feature_importances\n",
        "feature_importances = single_feature_analysis(train, 'GT_NO2', groups, n_splits)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:27:58.355125Z",
          "iopub.status.busy": "2024-07-14T13:27:58.354742Z",
          "iopub.status.idle": "2024-07-14T13:27:58.388466Z",
          "shell.execute_reply": "2024-07-14T13:27:58.38673Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.355094Z"
        },
        "id": "cu-5huLAA2k9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "hshsd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-14T13:27:58.389612Z",
          "iopub.status.idle": "2024-07-14T13:27:58.390052Z",
          "shell.execute_reply": "2024-07-14T13:27:58.389886Z",
          "shell.execute_reply.started": "2024-07-14T13:27:58.389868Z"
        },
        "id": "fqBHloMqA2k9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.fit(train.drop(columns='GT_NO2'),train['GT_NO2'])\n",
        "y = model.predict(test)\n",
        "smaple = pd.read_csv('/Users/ahmed/Downloads/computerscience/Kaggle/geoai-ground-level-no2-estimation/geoai-ground/data/SampleSubmission.csv')\n",
        "smaple['GT_NO2'] = post_process(test,y)\n",
        "smaple.to_csv('submissionpost2.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:32:00.517132Z",
          "iopub.status.busy": "2024-07-14T13:32:00.516715Z",
          "iopub.status.idle": "2024-07-14T13:32:11.521785Z",
          "shell.execute_reply": "2024-07-14T13:32:11.52073Z",
          "shell.execute_reply.started": "2024-07-14T13:32:00.517099Z"
        },
        "id": "cp0d4T3DA2k-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.fit(train.drop(columns='GT_NO2'),train['GT_NO2'])\n",
        "y_pred = model.predict(test)\n",
        "sub_df = pd.DataFrame({'id': test_id,'GT_NO2':y_pred})\n",
        "sub_df.to_csv('submission983178851079467.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:33:07.383755Z",
          "iopub.status.busy": "2024-07-14T13:33:07.383329Z",
          "iopub.status.idle": "2024-07-14T13:33:07.689903Z",
          "shell.execute_reply": "2024-07-14T13:33:07.688533Z",
          "shell.execute_reply.started": "2024-07-14T13:33:07.383723Z"
        },
        "id": "9m6QE2TgA2k-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "importances = model.feature_importances_\n",
        "names = model.feature_names_\n",
        "fi = pd.DataFrame({'Feature': names,\n",
        "                   'importances': importances})\n",
        "fi = fi.sort_values(by='importances', ascending=False)\n",
        "\n",
        "fi.plot(kind='bar', x='Feature', y='importances', legend=False, figsize=(10, 6))\n",
        "plt.title('Feature Importances')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-14T13:33:32.869208Z",
          "iopub.status.busy": "2024-07-14T13:33:32.86811Z",
          "iopub.status.idle": "2024-07-14T13:33:32.876484Z",
          "shell.execute_reply": "2024-07-14T13:33:32.875389Z",
          "shell.execute_reply.started": "2024-07-14T13:33:32.869166Z"
        },
        "id": "NLSlX2mtA2k-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yLOUsVtXwyQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 5270243,
          "sourceId": 8770223,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30746,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
